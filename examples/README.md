# Performance Lab Example Workflows

This directory contains example ComfyUI workflows demonstrating the Performance Lab nodes.

## Available Workflows

### ‚úÖ `performance_lab_v2.json` (CURRENT - v2.0)

**Status:** ‚úÖ Fully compatible with v2.0

**Description:** Complete demonstration of all 11 v2.0 nodes including:
- Basic performance monitoring (Timer, Report, VRAM Monitor)
- Auto GPU detection and suggestions (AutoFix)
- LLM-powered optimization (Optimizer, A/B Test)
- Preference learning (Feedback)
- Network discovery and LiteLLM setup (NetworkSetup)

**Use this workflow to:**
1. Learn how to use all v2.0 features
2. Copy individual node groups for your own workflows
3. Test Performance Lab functionality

**Quick Start:**
```
1. Load this workflow in ComfyUI
2. Connect your actual workflow between the Timer and Report nodes
3. Connect AutoFix outputs to your KSampler/EmptyLatent
4. Run to get performance metrics and GPU-aware suggestions
```

---

### ‚ùå `performance_lab_v0.9_legacy.json` (DEPRECATED)

**Status:** ‚ùå NOT compatible with v2.0 (LEGACY)

**Description:** Old v0.9 workflow using nodes that were removed in v2.0:
- `PerfLab_OptimizationLoop` ‚Üí Use `PerfLab_Optimizer` + `PerfLab_ABTest`
- `PerfLab_OneClickOptimize` ‚Üí Use `PerfLab_AutoFix`
- `PerfLab_AutoDetectGPU` ‚Üí Use `PerfLab_AutoFix`
- `PerfLab_QuickStart` ‚Üí See README.md and workflow descriptions
- `PerfLab_SmartPrompt` ‚Üí Use `PerfLab_Optimizer`

**Migration Guide:**
If you have old workflows, replace nodes as follows:

| Old Node (v0.9) | New Node (v2.0) | Notes |
|-----------------|-----------------|-------|
| OptimizationLoop | LLM Optimizer | More flexible, works with any LLM |
| OneClickOptimize | AutoFix | Automatic GPU detection built-in |
| AutoDetectGPU | AutoFix | Combined into one node |
| QuickStart | (removed) | Use node descriptions instead |
| SmartPrompt | LLM Optimizer | More powerful prompting |

---

## Node Categories

### Core Utility Nodes (6)
- ‚è±Ô∏è **Start Timer** - Place at workflow start
- üìä **Performance Report** - Place at workflow end
- üíæ **VRAM Monitor** - Check memory anywhere
- üìù **Show Text** - Display outputs
- üìê **Cap Resolution** - Limit size for testing
- üìä **Compare Results** - Before/after comparison

### LLM-Powered Nodes (5)
- ü™Ñ **AutoFix** - Drop anywhere for auto GPU detection
- üß† **LLM Optimizer** - KoboldCPP/Ollama integration
- üî¨ **A/B Test** - Compare configurations
- üëç **Record Preference** - Train the system
- üåê **Network Setup** - Multi-machine discovery

---

## Usage Tips

### Basic Monitoring (No LLM)
```
[Timer] ‚Üí [Your Workflow] ‚Üí [Report] ‚Üí [ShowText]
```

### Auto GPU Suggestions (No LLM)
```
[AutoFix] ‚Üí Connect outputs to:
  - suggested_steps ‚Üí KSampler steps
  - suggested_resolution ‚Üí EmptyLatent width/height
  - suggested_cfg ‚Üí KSampler cfg
```

### LLM-Guided Optimization
```
[Timer] ‚Üí [Workflow] ‚Üí [Report] ‚Üí [LLM Optimizer] ‚Üí [ShowText]
                                         ‚Üì
Enter issue: "Images too dark"
Get: Specific parameter fixes
```

### A/B Testing
```
[Report A] ‚îÄ‚î¨‚îÄ‚Üí [Compare Results] ‚Üí See % improvement
[Report B] ‚îÄ‚îò

[Config A] ‚îÄ‚î¨‚îÄ‚Üí [A/B Test] ‚Üí Get theoretical speedup
[Config B] ‚îÄ‚îò
```

---

## LLM Setup

Performance Lab works with:
- **KoboldCPP** (default: http://127.0.0.1:5001)
- **Ollama** (http://127.0.0.1:11434)
- **LiteLLM** (for load balancing multiple backends)

### Quick LLM Setup

**Option 1: KoboldCPP** (Recommended)
```bash
# Download and run KoboldCPP
./koboldcpp --model your_model.gguf --port 5001
```

**Option 2: Ollama**
```bash
ollama serve
ollama pull llama3.2
```

**Option 3: LiteLLM** (Multi-machine)
```bash
# Use Network Setup node to discover services
# Copy generated config to ~/.litellm/config.yaml
litellm --config ~/.litellm/config.yaml
# Then point LLM Optimizer to http://localhost:4000
```

---

## Creating Your Own Workflow

1. **Start Simple:**
   - Add Timer at start
   - Add Report at end
   - Run to get baseline metrics

2. **Add Auto-Suggestions:**
   - Drop AutoFix node anywhere
   - Connect outputs to your sampler settings
   - Get GPU-aware recommendations

3. **Optimize with LLM:**
   - Connect Report to LLM Optimizer
   - Describe your issue
   - Apply suggested changes

4. **Compare Results:**
   - Use Compare Results for before/after
   - Use A/B Test for configuration testing
   - Record Preference to train the system

---

## Version History

- **v2.0.0** - Current version (11 focused nodes)
- **v0.9.0** - Legacy version (deprecated, 31 nodes)

For more info, see the main [README.md](../README.md)
