# ComfyUI Performance Lab v2.0

**LLM-guided workflow optimization for ComfyUI.**

![Performance Lab](https://img.shields.io/badge/ComfyUI-Performance%20Lab-blue) ![Python 3.7+](https://img.shields.io/badge/Python-3.7+-green) ![Version](https://img.shields.io/badge/Version-2.0.0-orange)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   âš¡ COMFYUI PERFORMANCE LAB v2.0 âš¡                          â•‘
â•‘        Focused, LLM-guided workflow optimization (11 nodes, was 31)          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## What's New in v2.0

**Complete reimagining** - reduced from 31 bloated nodes to 11 focused nodes:

- **6 Core utility nodes** - Timer, Report, VRAMMonitor, ShowText, CapResolution, Compare
- **5 New LLM-powered nodes** - AutoFix, Optimizer, ABTest, Feedback, NetworkSetup

### Key Features

- **ğŸª„ AutoFix Node** - Drop anywhere, get automatic GPU-aware suggestions
- **ğŸ§  LLM Optimizer** - Connect to KoboldCPP/Ollama for intelligent recommendations
- **ğŸ”¬ A/B Testing** - Compare configurations side-by-side
- **ğŸ‘ Preference Learning** - System learns what you like over time
- **ğŸŒ Network Discovery** - Auto-find AI services on your network
- **ğŸ“Š LiteLLM Integration** - Load balance across multiple LLM backends

---

## Installation

### ComfyUI Custom Nodes

Place in `ComfyUI/custom_nodes/ComfyUI_PerformanceLab/`:

```bash
cd ComfyUI/custom_nodes
git clone https://github.com/laboratoiresonore/ComfyUI_PerformanceLab.git
```

Restart ComfyUI - you'll see 11 nodes in the **"âš¡ Performance Lab"** category.

---

## Quick Start

### 1. Basic Performance Monitoring

```
[â±ï¸ Start Timer] â†’ [Your Workflow...] â†’ [ğŸ“Š Performance Report]
        â†“                                          â†“
     timer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  (shows duration)
```

### 2. Auto-Optimization (No LLM Required)

Drop the **ğŸª„ AutoFix** node anywhere in your workflow:

```
[Any Node] â†’ [ğŸª„ AutoFix] â†’ [Next Node]
                   â†“
         Shows: GPU, VRAM, model type
         Outputs: suggested_steps, suggested_resolution, suggested_cfg
```

### 3. LLM-Guided Optimization

Connect to your local LLM for intelligent suggestions:

```
[ğŸ“Š Performance Report] â†’ [ğŸ§  LLM Optimizer]
                                   â†“
         Enter: "My images are too dark"
         Get: Specific parameter suggestions
```

---

## Node Reference

### Core Utility Nodes (6)

| Node | Description |
|------|-------------|
| **â±ï¸ Start Timer** | Place at workflow start for timing |
| **ğŸ“Š Performance Report** | Shows duration, peak VRAM at workflow end |
| **ğŸ’¾ VRAM Monitor** | Check GPU memory at any point (passthrough) |
| **ğŸ“ Show Text** | Display any text output |
| **ğŸ“ Cap Resolution** | Limit dimensions for faster testing |
| **ğŸ“Š Compare Results** | Before/after comparison with % change |

### LLM-Powered Nodes (5)

| Node | Description |
|------|-------------|
| **ğŸª„ AutoFix** | Drop anywhere - auto GPU detection & suggestions |
| **ğŸ§  LLM Optimizer** | Connect to Kobold/Ollama for smart suggestions |
| **ğŸ”¬ A/B Test** | Compare two configurations side-by-side |
| **ğŸ‘ Record Preference** | Record which option you preferred |
| **ğŸŒ Network Setup** | Discover services & generate LiteLLM config |

---

## LiteLLM Setup for Multi-Kobold Load Balancing

If you run multiple KoboldCPP instances on your network, use LiteLLM for automatic load balancing:

### 1. Install LiteLLM

```bash
pip install litellm[proxy]
```

### 2. Use Network Setup Node

Add the **ğŸŒ Network Setup** node to your workflow and run it:
- Automatically discovers Kobold/Ollama instances on your network
- Generates ready-to-use LiteLLM configuration

### 3. Save Configuration

Copy the generated YAML to `~/.litellm/config.yaml`:

```yaml
# Auto-generated by Performance Lab
model_list:
  - model_name: local-llm
    litellm_params:
      model: openai/kobold
      api_base: http://192.168.1.105:5001/v1
      rpm: 10
  - model_name: local-llm
    litellm_params:
      model: openai/kobold
      api_base: http://192.168.1.106:5001/v1
      rpm: 10

router_settings:
  routing_strategy: least-busy
  num_retries: 2
  allowed_fails: 3
  cooldown_time: 60
```

### 4. Start LiteLLM Proxy

```bash
litellm --config ~/.litellm/config.yaml
```

Now point the LLM Optimizer node at `http://localhost:4000` for automatic load balancing!

### Manual Configuration

If auto-discovery doesn't find your servers, manually create the config:

```yaml
model_list:
  # KoboldCPP instances
  - model_name: local-llm
    litellm_params:
      model: openai/kobold
      api_base: http://YOUR_IP_1:5001/v1
      rpm: 10

  # Ollama instances
  - model_name: local-llm
    litellm_params:
      model: ollama/llama3.2
      api_base: http://YOUR_IP_2:11434
      rpm: 10

router_settings:
  routing_strategy: least-busy  # or: latency-based, lowest-cost
  num_retries: 2
  allowed_fails: 3
  cooldown_time: 60
```

---

## Memory & Preference Learning

Performance Lab remembers your preferences in `~/.performance_lab/memory.json`:

- **CFG preferences** per model type
- **Quality vs speed** tradeoffs you prefer
- **Solutions that worked** for past issues

The more you use it, the better suggestions get!

### Reset Memory

```python
from performance_lab.memory import get_memory_manager
get_memory_manager().reset()
```

---

## File Structure

```
performance_lab/
â”œâ”€â”€ __init__.py           # 11 focused nodes (v2.0)
â”œâ”€â”€ memory.py             # Preference learning & storage
â”œâ”€â”€ discovery.py          # Network service discovery
â”œâ”€â”€ connection_pool.py    # HTTP connection pooling
â”œâ”€â”€ workflow_utils.py     # Workflow analysis utilities
â”œâ”€â”€ model_tuner.py        # Model detection
â”œâ”€â”€ mods/                 # Workflow modification presets
â”‚   â”œâ”€â”€ vram_optimizer.py
â”‚   â””â”€â”€ bypass_upscalers.py
â””â”€â”€ __init__v1_backup.py  # v1.0 backup (31 nodes)
```

---

## Migrating from v1.0

### Removed Nodes (Use AutoFix Instead)

These redundant nodes were replaced by the **AutoFix** node:
- OneClickOptimize, QuickStart, TestModeToggle
- AutoDetectGPU, ModelDetector
- ReduceSteps, ReduceBatch, OptimizeCFG
- SpeedPreset, LowVRAMPreset

### Removed Nodes (Rarely Used)

- Analyzer, BlackImageFix (use LLM Optimizer)
- GeneratePrompt, SmartPrompt, SmartOptimize (use LLM Optimizer)
- IterationTracker, OptimizationLoop (use A/B Test)
- Switch, IntSwitch, FloatSwitch (use ComfyUI's built-in)
- LoadWorkflow, QueueWorkflow, BenchmarkRunner (niche)
- EndpointHealth (use NetworkSetup)

### Kept Nodes

All core utility nodes remain: Timer, Report, VRAMMonitor, ShowText, CapResolution, Compare.

---

## Troubleshooting

### LLM Not Connecting

1. Check endpoint URL (default: `http://127.0.0.1:5001`)
2. Verify Kobold/Ollama is running: `curl http://localhost:5001/api/v1/info`
3. For Ollama, the endpoint should be `http://localhost:11434`

### Network Discovery Not Finding Services

1. Increase timeout: `timeout_ms: 500`
2. Check firewall settings on target machines
3. Verify services are bound to 0.0.0.0, not 127.0.0.1

### Memory Not Saving

Check write permissions for `~/.performance_lab/`

---

## Version History

**v2.0.0** - Complete Reimagining
- Reduced from 31 to 11 focused nodes
- New LLM-powered optimization nodes
- Preference learning and memory system
- Network service discovery
- LiteLLM integration for load balancing

**v1.x** - Legacy
- See `__init__v1_backup.py` for previous 31-node version

---

## License

MIT - Use freely, modify as needed.

---

Made with âš¡ for the ComfyUI community
