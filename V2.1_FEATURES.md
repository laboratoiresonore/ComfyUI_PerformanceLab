# Performance Lab v2.1 - Ultimate Edition

**The Perfect Tool for Launching, Monitoring, Testing & Optimizing ANY AI Workflow**

---

## üéØ What's New in v2.1

Performance Lab v2.1 transforms from a simple ComfyUI helper into the **ultimate AI workflow testing and optimization platform**. Now supports:

- **WhimWeaver Integration** - Launch and monitor WhimWeaver workflows
- **Multi-Modal Testing** - Test image, video, LLM, and audio generation
- **GPU Benchmarking** - Compare your GPU to others with real-world estimates
- **Batch Processing** - Run hundreds of variations from CSV files
- **Enhanced LLM Support** - LM Studio, Jan, KoboldCPP, Ollama, LiteLLM

---

## üìä Node Count

- **v1.0:** 31 nodes (bloated)
- **v2.0:** 11 nodes (focused)
- **v2.1:** 16 nodes (ultimate)

### New v2.1 Nodes (5)

| Node | Purpose |
|------|---------|
| üéØ **GPU Benchmark Hints** | Compare your GPU's performance to others |
| üìä **Batch Processor (CSV)** | Process hundreds of workflow variations |
| üöÄ **WhimWeaver Launcher** | Launch and control WhimWeaver |
| üìä **WhimWeaver Monitor** | Real-time WhimWeaver performance tracking |
| üé≠ **Multi-Modal Tester** | Universal testing for image/video/LLM/audio |

---

## üöÄ Complete Use Cases

### 1. Launch & Monitor WhimWeaver

```
[WhimWeaver Launcher] ‚Üí (action: launch)
         ‚Üì
[WhimWeaver Monitor] ‚Üí (mode: current) ‚Üí See CPU/RAM/GPU in real-time
         ‚Üì
[WhimWeaver Monitor] ‚Üí (mode: summary) ‚Üí Final performance report
```

**Perfect for:**
- Automated WhimWeaver testing
- Performance profiling
- Resource usage tracking
- Batch WhimWeaver processing

### 2. Test Any AI Model

```
[Timer] ‚Üí [Multi-Modal Tester] ‚Üí Automatically detects:
                ‚Üì                  ‚Ä¢ Image (SD, SDXL, Flux)
          Auto-detects type       ‚Ä¢ Video (AnimateDiff, SVD)
          Tracks throughput       ‚Ä¢ LLM (any text model)
          Gives recommendations   ‚Ä¢ Audio generation
```

**Perfect for:**
- Model comparisons
- Performance testing
- Throughput benchmarking
- Quality vs speed analysis

### 3. GPU Performance Hints

```
[GPU Benchmark Hints]
  Model: SDXL, Steps: 30
         ‚Üì
  "RTX 4090: 12.5s (your GPU)"
  "RTX 4080: 15.0s (+20% slower)"
  "A100: 9.4s (-25% faster)"
```

**Perfect for:**
- Hardware upgrade planning
- Cloud GPU selection
- Performance expectations
- Build recommendations

### 4. Batch Testing

```
[Batch Processor] ‚Üí action: create_template
         ‚Üì
  Creates: batch_config.csv
         ‚Üì
Edit CSV with 100 prompt variations
         ‚Üì
[Batch Processor] ‚Üí action: load
         ‚Üì
[Batch Processor] ‚Üí action: get_next (x100)
         ‚Üì
  All variations tested!
```

**Perfect for:**
- Prompt engineering
- Parameter tuning
- A/B testing
- Dataset generation

---

## üß† Enhanced LLM Support

The LLM Optimizer now tries **5 different endpoint types** automatically:

### Supported LLM Tools

| Tool | Endpoint | Auto-Detected |
|------|----------|---------------|
| **LM Studio** | http://localhost:1234 | ‚úÖ NEW |
| **Jan** | http://localhost:1337 | ‚úÖ NEW |
| **LiteLLM** | http://localhost:4000 | ‚úÖ NEW |
| **KoboldCPP** | http://localhost:5001 | ‚úÖ v2.0 |
| **Ollama** | http://localhost:11434 | ‚úÖ v2.0 |

### LLM Setup (Quick Start)

**Option 1: LM Studio** (Recommended - Easiest)
```bash
1. Download LM Studio from https://lmstudio.ai
2. Load any model (e.g., "mistral-7b-instruct")
3. Start local server (default port 1234)
4. In LLM Optimizer: http://localhost:1234
```

**Option 2: Jan** (Privacy-Focused)
```bash
1. Download Jan from https://jan.ai
2. Load model in Jan interface
3. Enable API server (Settings ‚Üí Advanced)
4. In LLM Optimizer: http://localhost:1337
```

**Option 3: KoboldCPP** (Advanced)
```bash
./koboldcpp --model your-model.gguf --port 5001
```

**Option 4: Ollama** (CLI-Friendly)
```bash
ollama serve
ollama pull llama3.2
```

**The LLM Optimizer will try ALL of these automatically!**

---

## üìã Complete Feature Matrix

### Image Generation

| Feature | Support |
|---------|---------|
| SDXL | ‚úÖ Full |
| Flux | ‚úÖ Full |
| SD 1.5 | ‚úÖ Full |
| SD3 | ‚úÖ Full |
| AutoFix suggestions | ‚úÖ Model-specific |
| GPU benchmarks | ‚úÖ Per-model timing |

### Video Generation

| Feature | Support |
|---------|---------|
| AnimateDiff | ‚úÖ Auto-detected |
| SVD | ‚úÖ Auto-detected |
| Frame-based timing | ‚úÖ Throughput tracking |
| Multi-Modal Tester | ‚úÖ Dedicated support |

### LLM Inference

| Feature | Support |
|---------|---------|
| KoboldCPP | ‚úÖ Full |
| Ollama | ‚úÖ Full |
| LM Studio | ‚úÖ NEW v2.1 |
| Jan | ‚úÖ NEW v2.1 |
| LiteLLM | ‚úÖ NEW v2.1 |
| Token/sec tracking | ‚úÖ Multi-Modal Tester |

### WhimWeaver

| Feature | Support |
|---------|---------|
| Auto-discovery | ‚úÖ Searches common paths |
| Launch with args | ‚úÖ Custom arguments |
| Real-time monitoring | ‚úÖ CPU/RAM/GPU |
| Performance summary | ‚úÖ Post-completion |
| Multi-instance | ‚úÖ Track by PID |

---

## üéØ Workflow Examples

### Example 1: Test Image Model on Different GPUs

```
[Timer]
  ‚Üì
[GPU Benchmark Hints]
  model_type: sdxl
  steps: 30
  resolution: 1024
  ‚Üì
Shows: "Your GPU: 12.5s, RTX 4090 would be: 9.2s"
```

### Example 2: Batch Test 100 Prompts

**1. Create template:**
```
[Batch Processor]
  action: create_template
  template_type: image
  csv_path: prompts.csv
```

**2. Edit prompts.csv:**
```csv
prompt,negative_prompt,steps,cfg,resolution,seed
"a cat in space",ugly,30,7.0,1024,12345
"a dog on mars",blurry,25,6.5,768,67890
... (98 more rows)
```

**3. Process batch:**
```
[Batch Processor] action: load ‚Üí Loads 100 configs
  ‚Üì
[Loop 100x]
  [Batch Processor] action: get_next
    ‚Üì
  [Your Workflow] uses the config
    ‚Üì
  [Performance Report] tracks results
```

### Example 3: WhimWeaver Automated Testing

```
[WhimWeaver Launcher]
  action: launch
  workflow_path: /path/to/workflow.json
  ‚Üì
[WhimWeaver Monitor]
  mode: current
  ‚Üì
  Every 5 seconds:
    CPU: 45%
    RAM: 2.1GB
    GPU: 8.3GB
  ‚Üì
[WhimWeaver Monitor]
  mode: summary
  ‚Üì
  Total Runtime: 125s
  Avg CPU: 42%
  Peak RAM: 3.2GB
```

### Example 4: LLM-Guided Multi-Modal Optimization

```
[Timer] ‚Üí [Multi-Modal Tester]
  ‚Üì
[Report] ‚Üí [LLM Optimizer]
  user_message: "Images are too dark"
  llm_endpoint: http://localhost:1234 (LM Studio)
  ‚Üì
LLM suggests: "Increase CFG to 8.5, try negative_prompt: 'dark, underexposed'"
  ‚Üì
[A/B Test] Compare old vs new settings
  ‚Üì
[Feedback] Record which you prefer
  ‚Üì
System learns your preferences!
```

---

## üîß Installation

### Requirements

```bash
# Install psutil for WhimWeaver monitoring
pip install psutil

# Optional: LiteLLM for multi-LLM load balancing
pip install litellm[proxy]
```

Or use the provided file:
```bash
pip install -r requirements.txt
```

### ComfyUI Installation

```bash
cd ComfyUI/custom_nodes
git clone https://github.com/laboratoiresonore/ComfyUI_PerformanceLab.git
```

Restart ComfyUI and look for **"‚ö° Performance Lab"** category!

---

## üìñ API Reference

### GPU Benchmark Node

**Inputs:**
- `model_type`: flux | sdxl | sd15 | video
- `steps`: Number of sampling steps
- `resolution`: Image resolution (optional)
- `batch_size`: Batch size (optional)

**Outputs:**
- `benchmark_info`: Your GPU's performance
- `comparison`: vs other GPUs
- `estimated_time_sec`: Predicted generation time
- `recommendations`: Warnings about batch/resolution

### Batch Processor Node

**Inputs:**
- `csv_path`: Path to CSV file
- `action`: load | get_next | get_status | create_template
- `template_type`: image | video | llm (for template creation)

**Outputs:**
- `status`: Current operation status
- `total_configs`: Total configurations in batch
- `current_config_json`: Current config (JSON string)
- `next_config_json`: Next config (JSON string)

### WhimWeaver Launcher Node

**Inputs:**
- `action`: launch | stop | status
- `whimweaver_path`: Path to WhimWeaver (optional, auto-detected)
- `workflow_path`: Workflow JSON to load (optional)
- `extra_args`: Command-line arguments (optional)

**Outputs:**
- `status`: Operation result
- `is_running`: Boolean running state
- `process_id`: PID for monitoring

### WhimWeaver Monitor Node

**Inputs:**
- `mode`: current | summary | attach_pid
- `pid`: Process ID (for attach_pid mode)

**Outputs:**
- `current_metrics`: Real-time metrics (current mode)
- `cpu_percent`: CPU usage
- `memory_mb`: RAM usage
- `gpu_memory_mb`: GPU VRAM usage
- `summary`: Final performance summary (summary mode)

### Multi-Modal Tester Node

**Inputs:**
- `model_type`: auto | image | video | llm | audio
- `workflow_path`: Path to workflow for auto-detection (optional)
- `trigger`: Any input to trigger test (optional)
- `timer`: Timer from Start Timer node (optional)

**Outputs:**
- `test_report`: Comprehensive test results
- `duration_sec`: Total duration
- `throughput`: Model-specific throughput (images/sec, frames/sec, tokens/sec)
- `model_type`: Detected or specified type
- `recommendations`: Optimization suggestions

---

## üéì Best Practices

### 1. GPU Benchmarking

‚úÖ **DO:**
- Run benchmarks before buying new GPU
- Check if your settings exceed GPU limits
- Use estimated times for planning

‚ùå **DON'T:**
- Compare different model types
- Expect exact times (these are estimates)

### 2. Batch Processing

‚úÖ **DO:**
- Start with small batches (10-20)
- Use templates for consistency
- Track results in CSV

‚ùå **DON'T:**
- Load 1000+ configs at once
- Mix incompatible parameters
- Forget to save results

### 3. WhimWeaver Integration

‚úÖ **DO:**
- Stop previous instance before launching new one
- Monitor during long runs
- Check summary after completion

‚ùå **DON'T:**
- Launch multiple instances simultaneously
- Forget to attach monitor to correct PID

### 4. LLM Optimization

‚úÖ **DO:**
- Use specific, detailed questions
- Include performance report context
- Try multiple LLM tools

‚ùå **DON'T:**
- Use vague questions like "optimize this"
- Expect LLM to fix code bugs
- Ignore LLM when no endpoint is running

---

## üêõ Troubleshooting

### "WhimWeaver integration module not available"
**Cause:** psutil not installed
**Fix:** `pip install psutil`

### "Could not connect to LLM"
**Cause:** No LLM server running
**Fix:** Start one of: LM Studio, Jan, KoboldCPP, Ollama
**Check:** Try all default ports automatically attempted

### "GPU not in benchmark database"
**Cause:** Uncommon GPU
**Fix:** Still works! Uses conservative estimates
**Note:** Open an issue to add your GPU to the database

### "No batch loaded"
**Cause:** Forgot to run `action: load`
**Fix:** Load CSV first before `get_next`

### "WhimWeaver not found"
**Cause:** Auto-discovery failed
**Fix:** Specify full path in `whimweaver_path` input

---

## üîÆ Future Plans (v2.2+)

- Cloud GPU integration (RunPod, Vast.ai)
- Workflow marketplace integration
- Advanced batch scheduling
- Multi-GPU load balancing
- Real-time collaboration features
- Mobile monitoring app
- Discord/Slack notifications

---

## üôè Credits

**Author:** Laboratoire Sonore
**Version:** 2.1.0
**License:** MIT
**Repository:** https://github.com/laboratoiresonore/ComfyUI_PerformanceLab

### Powered By:

- **psutil** - Process monitoring
- **LM Studio** - Local LLM inference
- **KoboldCPP** - GGUF model serving
- **Ollama** - Model management
- **LiteLLM** - Multi-LLM proxy
- **ComfyUI** - The amazing workflow platform

---

## üìù Changelog

### v2.1.0 (2026-01-10) - Ultimate Edition

**New Features:**
- ‚ú® WhimWeaver launcher and monitor nodes
- ‚ú® Multi-modal testing framework (image/video/LLM/audio)
- ‚ú® GPU benchmark database with performance hints
- ‚ú® Batch CSV processor for workflow variations
- ‚ú® LM Studio and Jan support in LLM Optimizer
- ‚ú® Enhanced error messages with all supported endpoints

**Improvements:**
- üöÄ LLM Optimizer now tries 5 endpoint types automatically
- üöÄ Better GPU detection and recommendations
- üöÄ Process monitoring with psutil
- üöÄ Workflow type auto-detection

**Bug Fixes:**
- üêõ Fixed F-string syntax errors (v2.0.1)
- üêõ Improved error handling in all nodes

### v2.0.0 (2024-01-09) - Complete Reimagining

- Reduced from 31 bloated nodes to 11 focused nodes
- New LLM-powered optimization
- Preference learning and memory
- Network service discovery

### v1.0.0 - Initial Release

- 31 nodes for various optimizations
- Basic performance monitoring

---

Made with ‚ö° for the ComfyUI and WhimWeaver communities
