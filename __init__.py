"""
ComfyUI Performance Lab - Iterative workflow optimization with AI assistance.

This node pack provides comprehensive optimization tools:
- Performance monitoring (timing, VRAM tracking)
- Quick optimizations (resolution cap, bypass upscalers, etc.)
- Workflow analysis and diagnostics
- LLM prompt generation for AI-assisted optimization
- Before/after comparison

Install: Place in ComfyUI/custom_nodes/ComfyUI_PerformanceLab/
"""

import os
import sys
import json
import time
import copy
import re
from typing import Dict, Any, List, Tuple, Optional

# Version and metadata
__version__ = "0.8.0"
__author__ = "Laboratoire Sonore"
__description__ = "ComfyUI Performance Lab - Iterative workflow optimization with AI assistance"

# ComfyUI Manager integration - explicit exports
# WEB_DIRECTORY is None because we don't have custom web assets
WEB_DIRECTORY = None

# __all__ defines what's exported when using "from package import *"
__all__ = [
    "__version__",
    "NODE_CLASS_MAPPINGS",
    "NODE_DISPLAY_NAME_MAPPINGS",
    "WEB_DIRECTORY",
]

# Ensure the module directory is in path for imports
MODULE_DIR = os.path.dirname(os.path.abspath(__file__))
if MODULE_DIR not in sys.path:
    sys.path.insert(0, MODULE_DIR)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PERFORMANCE MONITORING NODES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class PerfLab_Timer:
    """Start Timer - Place at the START of your workflow."""

    DESCRIPTION = """‚è±Ô∏è START TIMER

HOW TO USE:
1. Add this node at the BEGINNING of your workflow
2. Connect the 'timer' output to a Performance Report node at the END
3. Run your workflow - the report will show how long it took

TIPS:
‚Ä¢ No inputs needed - just add it and connect
‚Ä¢ Works with any workflow
‚Ä¢ Pair with üìä Performance Report to see results"""

    CATEGORY = "‚ö° Performance Lab/Monitoring"
    FUNCTION = "start"
    RETURN_TYPES = ("PERF_TIMER",)
    RETURN_NAMES = ("timer",)

    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {}}

    def start(self):
        return ({"start_time": time.time(), "vram_readings": []},)


class PerfLab_Report:
    """Performance Report - Shows timing and VRAM results."""

    DESCRIPTION = """üìä PERFORMANCE REPORT

HOW TO USE:
1. Add this node at the END of your workflow
2. Connect ANY output from your last node to 'trigger'
3. (Optional) Connect a Timer node to 'timer' for accurate timing
4. Run workflow - results appear in console and outputs

OUTPUTS:
‚Ä¢ report: Text summary of performance
‚Ä¢ duration_sec: Time in seconds (for Compare node)
‚Ä¢ peak_vram_gb: Peak VRAM used (for Compare node)

TIPS:
‚Ä¢ Connect final image/latent to 'trigger' input
‚Ä¢ Use with ‚è±Ô∏è Start Timer for accurate timing
‚Ä¢ Feed outputs to üìä Compare Results for before/after"""

    CATEGORY = "‚ö° Performance Lab/Monitoring"
    FUNCTION = "report"
    RETURN_TYPES = ("STRING", "FLOAT", "FLOAT")
    RETURN_NAMES = ("report", "duration_sec", "peak_vram_gb")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "trigger": ("*", {"tooltip": "Connect ANY output here to trigger the report"}),
            },
            "optional": {
                "timer": ("PERF_TIMER", {"tooltip": "Connect from ‚è±Ô∏è Start Timer for accurate timing"}),
                "label": ("STRING", {"default": "Generation", "tooltip": "Name for this measurement"}),
            }
        }

    def report(self, trigger, timer=None, label="Generation"):
        end_time = time.time()

        # Calculate duration
        if timer and "start_time" in timer:
            duration = end_time - timer["start_time"]
        else:
            duration = 0.0

        # Get VRAM info
        peak_vram = 0.0
        current_vram = 0.0
        try:
            import torch
            if torch.cuda.is_available():
                peak_vram = torch.cuda.max_memory_allocated() / (1024**3)
                current_vram = torch.cuda.memory_allocated() / (1024**3)
                torch.cuda.reset_peak_memory_stats()
        except:
            pass

        # Build report
        report_lines = [
            f"‚ïê‚ïê‚ïê {label} Complete ‚ïê‚ïê‚ïê",
            f"‚è±Ô∏è  Duration: {duration:.2f} seconds",
            f"üìà Peak VRAM: {peak_vram:.2f} GB",
            f"üìä Current VRAM: {current_vram:.2f} GB",
        ]

        report = "\n".join(report_lines)
        print(f"\n[Performance Lab]\n{report}\n")

        return (report, duration, peak_vram)


class PerfLab_VRAMMonitor:
    """VRAM Monitor - Check GPU memory at any point."""

    DESCRIPTION = """üíæ VRAM MONITOR

HOW TO USE:
1. Place this node ANYWHERE in your workflow
2. Connect any data to 'passthrough' (it passes through unchanged)
3. Run workflow - VRAM info prints to console

OUTPUTS:
‚Ä¢ vram_info: Text showing used/free/total VRAM
‚Ä¢ used_gb: VRAM currently in use
‚Ä¢ free_gb: Available VRAM
‚Ä¢ passthrough: Your input, unchanged

TIPS:
‚Ä¢ Place between nodes to find which uses most VRAM
‚Ä¢ Add checkpoint_name to label the measurement
‚Ä¢ Passthrough lets you insert without breaking connections"""

    CATEGORY = "‚ö° Performance Lab/Monitoring"
    FUNCTION = "check"
    RETURN_TYPES = ("STRING", "FLOAT", "FLOAT", "*")
    RETURN_NAMES = ("vram_info", "used_gb", "free_gb", "passthrough")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {},
            "optional": {
                "passthrough": ("*", {"tooltip": "Connect any data - it passes through unchanged"}),
                "checkpoint_name": ("STRING", {"default": "", "tooltip": "Label for this measurement point"}),
            }
        }

    def check(self, passthrough=None, checkpoint_name=""):
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                used = torch.cuda.memory_allocated(0) / (1024**3)
                reserved = torch.cuda.memory_reserved(0) / (1024**3)
                free = total - reserved

                label = f" [{checkpoint_name}]" if checkpoint_name else ""
                info = f"üíæ VRAM{label}: {used:.1f}GB used / {free:.1f}GB free / {total:.1f}GB total ({gpu_name})"
                print(f"[Performance Lab] {info}")
                return (info, used, free, passthrough)
            else:
                return ("No CUDA GPU", 0.0, 0.0, passthrough)
        except Exception as e:
            return (f"Error: {e}", 0.0, 0.0, passthrough)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# QUICK OPTIMIZATION NODES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class PerfLab_CapResolution:
    """Cap Resolution - Limit dimensions for faster testing."""

    DESCRIPTION = """üìê CAP RESOLUTION

HOW TO USE:
1. Connect your width/height values to this node
2. Set 'max_size' to your target (e.g., 768)
3. Connect outputs to Empty Latent or other nodes
4. Toggle 'enabled' to quickly compare original vs capped

INPUTS:
‚Ä¢ width/height: Your original dimensions
‚Ä¢ max_size: Maximum allowed dimension
‚Ä¢ enabled: Turn optimization on/off

WHY USE THIS:
‚Ä¢ Resolution affects VRAM quadratically (2x res = 4x VRAM)
‚Ä¢ 768px is great for testing, use 1024+ for finals
‚Ä¢ Keeps aspect ratio intact"""

    CATEGORY = "‚ö° Performance Lab/Quick Optimize"
    FUNCTION = "cap"
    RETURN_TYPES = ("INT", "INT")
    RETURN_NAMES = ("width", "height")

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "width": ("INT", {"default": 1024, "min": 64, "max": 8192,
                         "tooltip": "Original width - connect from your workflow or type a value"}),
                "height": ("INT", {"default": 1024, "min": 64, "max": 8192,
                          "tooltip": "Original height - connect from your workflow or type a value"}),
                "max_size": ("INT", {"default": 768, "min": 256, "max": 2048,
                            "tooltip": "Maximum dimension - 768 for testing, 1024+ for finals"}),
            },
            "optional": {
                "enabled": ("BOOLEAN", {"default": True,
                           "tooltip": "ON = cap resolution, OFF = pass through original"}),
            }
        }

    def cap(self, width: int, height: int, max_size: int, enabled: bool = True):
        if not enabled:
            return (width, height)

        # Scale down proportionally if needed
        if width > max_size or height > max_size:
            scale = max_size / max(width, height)
            new_width = int(width * scale) // 8 * 8  # Keep divisible by 8
            new_height = int(height * scale) // 8 * 8
            print(f"[Performance Lab] Resolution capped: {width}x{height} ‚Üí {new_width}x{new_height}")
            return (new_width, new_height)

        return (width, height)


class PerfLab_ReduceSteps:
    """Reduce Steps - Lower sampling steps for faster iteration."""

    DESCRIPTION = """üî¢ REDUCE STEPS

HOW TO USE:
1. Connect your steps value to this node
2. Set 'max_steps' to your testing limit (e.g., 20)
3. Connect output to your KSampler
4. Toggle 'enabled' to compare fast vs quality

INPUTS:
‚Ä¢ steps: Your original step count
‚Ä¢ max_steps: Maximum allowed steps
‚Ä¢ enabled: Turn optimization on/off

WHY USE THIS:
‚Ä¢ Steps scale linearly with time (2x steps = 2x time)
‚Ä¢ 15-20 steps is enough to check composition
‚Ä¢ Use full steps only for final renders"""

    CATEGORY = "‚ö° Performance Lab/Quick Optimize"
    FUNCTION = "reduce"
    RETURN_TYPES = ("INT",)
    RETURN_NAMES = ("steps",)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "steps": ("INT", {"default": 30, "min": 1, "max": 200,
                         "tooltip": "Your original step count - connect from workflow or enter value"}),
                "max_steps": ("INT", {"default": 20, "min": 1, "max": 100,
                             "tooltip": "Maximum steps during testing - 15-20 is usually enough"}),
            },
            "optional": {
                "enabled": ("BOOLEAN", {"default": True,
                           "tooltip": "ON = limit steps, OFF = use original value"}),
            }
        }

    def reduce(self, steps: int, max_steps: int, enabled: bool = True):
        if not enabled:
            return (steps,)

        if steps > max_steps:
            print(f"[Performance Lab] Steps reduced: {steps} ‚Üí {max_steps}")
            return (max_steps,)
        return (steps,)


class PerfLab_ReduceBatch:
    """Reduce Batch - Force batch size to 1 for VRAM savings."""

    DESCRIPTION = """üì¶ REDUCE BATCH

HOW TO USE:
1. Connect your batch_size value to this node
2. Enable 'force_single' to always use batch=1
3. Connect output to Empty Latent Image

INPUTS:
‚Ä¢ batch_size: Your original batch size
‚Ä¢ force_single: Force to 1 when enabled

WHY USE THIS:
‚Ä¢ Batch size multiplies VRAM usage directly
‚Ä¢ Batch=4 uses 4x the VRAM of batch=1
‚Ä¢ Essential for low VRAM GPUs
‚Ä¢ Disable for final batch renders"""

    CATEGORY = "‚ö° Performance Lab/Quick Optimize"
    FUNCTION = "reduce"
    RETURN_TYPES = ("INT",)
    RETURN_NAMES = ("batch_size",)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "batch_size": ("INT", {"default": 4, "min": 1, "max": 64,
                              "tooltip": "Your original batch size - each image uses more VRAM"}),
            },
            "optional": {
                "force_single": ("BOOLEAN", {"default": True,
                                "tooltip": "ON = force batch=1 (saves VRAM), OFF = keep original"}),
            }
        }

    def reduce(self, batch_size: int, force_single: bool = True):
        if force_single and batch_size > 1:
            print(f"[Performance Lab] Batch size reduced: {batch_size} ‚Üí 1")
            return (1,)
        return (batch_size,)


class PerfLab_OptimizeCFG:
    """Optimize CFG - Auto-adjust CFG for your model type."""

    DESCRIPTION = """üéØ OPTIMIZE CFG

HOW TO USE:
1. Select your model_type from the dropdown
2. Connect cfg output to your KSampler
3. Enable 'auto_adjust' to use optimal values

OPTIMAL CFG BY MODEL:
‚Ä¢ SD 1.5: 7.5
‚Ä¢ SDXL: 7.0
‚Ä¢ SD3: 4.5
‚Ä¢ Flux Dev: 3.5
‚Ä¢ Flux Schnell: 1.0

WHY USE THIS:
‚Ä¢ Wrong CFG causes black/burned images
‚Ä¢ Flux needs LOW CFG (1-3.5)
‚Ä¢ SD models need MEDIUM CFG (5-8)
‚Ä¢ Select 'Custom' to use your own value"""

    CATEGORY = "‚ö° Performance Lab/Quick Optimize"
    FUNCTION = "optimize"
    RETURN_TYPES = ("FLOAT",)
    RETURN_NAMES = ("cfg",)

    MODEL_CFG = {
        "SD 1.5": 7.5,
        "SDXL": 7.0,
        "SD3": 4.5,
        "Flux Dev": 3.5,
        "Flux Schnell": 1.0,
        "Custom": 7.0,
    }

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "cfg": ("FLOAT", {"default": 7.0, "min": 0.0, "max": 30.0, "step": 0.5,
                       "tooltip": "Your CFG value - will be adjusted based on model type"}),
                "model_type": (list(cls.MODEL_CFG.keys()),
                              {"tooltip": "Select your model - Flux needs low CFG, SD needs higher"}),
            },
            "optional": {
                "auto_adjust": ("BOOLEAN", {"default": True,
                               "tooltip": "ON = use optimal CFG for model, OFF = use your value"}),
            }
        }

    def optimize(self, cfg: float, model_type: str, auto_adjust: bool = True):
        if auto_adjust and model_type != "Custom":
            optimal = self.MODEL_CFG.get(model_type, cfg)
            if cfg != optimal:
                print(f"[Performance Lab] CFG adjusted for {model_type}: {cfg} ‚Üí {optimal}")
            return (optimal,)
        return (cfg,)


class PerfLab_SpeedPreset:
    """Speed Preset - All optimizations in one node."""

    DESCRIPTION = """üöÄ SPEED TEST PRESET

HOW TO USE:
1. Connect your width, height, steps, cfg
2. Connect outputs to your workflow nodes
3. Toggle 'enabled' to switch between test/production

WHAT IT DOES:
‚Ä¢ Caps resolution to target (default 512px)
‚Ä¢ Limits steps to target (default 15)
‚Ä¢ Keeps CFG unchanged

OUTPUTS:
‚Ä¢ width/height: Capped dimensions
‚Ä¢ steps: Limited step count
‚Ä¢ cfg: Passed through

USE WHEN:
‚Ä¢ Testing workflow changes quickly
‚Ä¢ Iterating on prompts/composition
‚Ä¢ Debugging errors (fast feedback)"""

    CATEGORY = "‚ö° Performance Lab/Quick Optimize"
    FUNCTION = "apply"
    RETURN_TYPES = ("INT", "INT", "INT", "FLOAT")
    RETURN_NAMES = ("width", "height", "steps", "cfg")

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "width": ("INT", {"default": 1024, "min": 64, "max": 8192,
                         "tooltip": "Original width from your workflow"}),
                "height": ("INT", {"default": 1024, "min": 64, "max": 8192,
                          "tooltip": "Original height from your workflow"}),
                "steps": ("INT", {"default": 30, "min": 1, "max": 200,
                         "tooltip": "Original steps from your workflow"}),
                "cfg": ("FLOAT", {"default": 7.0, "min": 0.0, "max": 30.0,
                       "tooltip": "CFG passes through unchanged"}),
            },
            "optional": {
                "enabled": ("BOOLEAN", {"default": True,
                           "tooltip": "ON = fast testing mode, OFF = original values"}),
                "target_resolution": ("INT", {"default": 512, "min": 256, "max": 1024,
                                     "tooltip": "Max resolution during speed test"}),
                "target_steps": ("INT", {"default": 15, "min": 4, "max": 50,
                                "tooltip": "Max steps during speed test"}),
            }
        }

    def apply(self, width, height, steps, cfg, enabled=True, target_resolution=512, target_steps=15):
        if not enabled:
            return (width, height, steps, cfg)

        # Cap resolution
        if width > target_resolution or height > target_resolution:
            scale = target_resolution / max(width, height)
            width = int(width * scale) // 8 * 8
            height = int(height * scale) // 8 * 8

        # Cap steps
        steps = min(steps, target_steps)

        print(f"[Performance Lab] Speed preset applied: {width}x{height}, {steps} steps")
        return (width, height, steps, cfg)


class PerfLab_LowVRAMPreset:
    """Low VRAM Preset - Optimized for 6GB/8GB/12GB GPUs."""

    DESCRIPTION = """üíæ LOW VRAM PRESET

HOW TO USE:
1. Connect width, height, steps, batch_size
2. Select your GPU's VRAM from dropdown
3. Connect outputs to your workflow

VRAM PRESETS:
‚Ä¢ 6GB: 512px max, 20 steps, batch=1
‚Ä¢ 8GB: 768px max, 25 steps, batch=1
‚Ä¢ 12GB: 1024px max, 30 steps, batch=1

OUTPUTS:
‚Ä¢ width/height: VRAM-safe dimensions
‚Ä¢ steps: Reasonable step count
‚Ä¢ batch_size: Always 1 for safety

USE WHEN:
‚Ä¢ Getting CUDA out of memory errors
‚Ä¢ Running SDXL on 8GB GPU
‚Ä¢ Want safe defaults for your card"""

    CATEGORY = "‚ö° Performance Lab/Quick Optimize"
    FUNCTION = "apply"
    RETURN_TYPES = ("INT", "INT", "INT", "INT")
    RETURN_NAMES = ("width", "height", "steps", "batch_size")

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "width": ("INT", {"default": 1024, "min": 64, "max": 8192,
                         "tooltip": "Original width from your workflow"}),
                "height": ("INT", {"default": 1024, "min": 64, "max": 8192,
                          "tooltip": "Original height from your workflow"}),
                "steps": ("INT", {"default": 30, "min": 1, "max": 200,
                         "tooltip": "Original steps from your workflow"}),
                "batch_size": ("INT", {"default": 4, "min": 1, "max": 64,
                              "tooltip": "Original batch size from your workflow"}),
            },
            "optional": {
                "enabled": ("BOOLEAN", {"default": True,
                           "tooltip": "ON = apply VRAM limits, OFF = use original values"}),
                "vram_target": (["6GB", "8GB", "12GB"],
                               {"tooltip": "Select your GPU's VRAM capacity"}),
            }
        }

    VRAM_SETTINGS = {
        "6GB": {"max_res": 512, "max_steps": 20},
        "8GB": {"max_res": 768, "max_steps": 25},
        "12GB": {"max_res": 1024, "max_steps": 30},
    }

    def apply(self, width, height, steps, batch_size, enabled=True, vram_target="8GB"):
        if not enabled:
            return (width, height, steps, batch_size)

        settings = self.VRAM_SETTINGS.get(vram_target, self.VRAM_SETTINGS["8GB"])
        max_res = settings["max_res"]
        max_steps = settings["max_steps"]

        # Cap resolution
        if width > max_res or height > max_res:
            scale = max_res / max(width, height)
            width = int(width * scale) // 8 * 8
            height = int(height * scale) // 8 * 8

        # Cap steps and force batch=1
        steps = min(steps, max_steps)
        batch_size = 1

        print(f"[Performance Lab] {vram_target} preset: {width}x{height}, {steps} steps, batch=1")
        return (width, height, steps, batch_size)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ANALYSIS & DIAGNOSTIC NODES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class PerfLab_Analyzer:
    """Workflow Analyzer - Analyze workflow and get suggestions."""

    DESCRIPTION = """üîç WORKFLOW ANALYZER

HOW TO USE:
1. Export your workflow as JSON (Ctrl+S in ComfyUI)
2. Paste the JSON into this node
3. Read the analysis and suggestions

OUTPUTS:
‚Ä¢ analysis: Node counts, features detected
‚Ä¢ suggestions: Specific optimization tips

DETECTS:
‚Ä¢ Upscaling (ESRGAN, etc.)
‚Ä¢ ControlNet usage
‚Ä¢ Model type (SDXL, Flux, etc.)
‚Ä¢ Video generation nodes

USE WHEN:
‚Ä¢ Starting optimization on a new workflow
‚Ä¢ Want to understand workflow complexity
‚Ä¢ Need suggestions for what to optimize"""

    CATEGORY = "‚ö° Performance Lab/Analysis"
    FUNCTION = "analyze"
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("analysis", "suggestions")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "workflow_json": ("STRING", {
                    "multiline": True,
                    "default": '{"nodes": [], "links": []}'
                }),
            }
        }

    def analyze(self, workflow_json: str):
        try:
            workflow = json.loads(workflow_json)
            nodes = workflow.get("nodes", [])
            links = workflow.get("links", [])

            # Count node types
            node_types = {}
            for node in nodes:
                ntype = node.get("type", "Unknown")
                node_types[ntype] = node_types.get(ntype, 0) + 1

            # Detect features and issues
            features = []
            suggestions = []
            node_str = " ".join(node_types.keys()).lower()

            # Feature detection
            if "upscale" in node_str or "esrgan" in node_str:
                features.append("Upscaling")
                suggestions.append("üí° Bypass upscalers during testing to save 2-4GB VRAM")

            if "controlnet" in node_str:
                features.append("ControlNet")
                suggestions.append("üí° ControlNet adds VRAM overhead - disable during initial tests")

            if "sdxl" in node_str or "xl" in node_str:
                features.append("SDXL")
                suggestions.append("üí° SDXL uses ~2x VRAM vs SD1.5 - use 1024x1024 max")

            if "flux" in node_str:
                features.append("Flux")
                suggestions.append("üí° Flux works best with CFG 1-3.5")

            if "video" in node_str or "animatediff" in node_str:
                features.append("Video")
                suggestions.append("üí° Video generation is very VRAM intensive - reduce frame count")

            # Build analysis
            analysis_lines = [
                "‚ïê‚ïê‚ïê Workflow Analysis ‚ïê‚ïê‚ïê",
                f"üìä Total Nodes: {len(nodes)}",
                f"üîó Total Links: {len(links)}",
                f"‚ú® Features: {', '.join(features) if features else 'Basic SD'}",
                "",
                "üìã Node Types:",
            ]

            for ntype, count in sorted(node_types.items(), key=lambda x: -x[1])[:10]:
                analysis_lines.append(f"   ‚Ä¢ {ntype}: {count}")

            analysis = "\n".join(analysis_lines)
            suggestions_text = "\n".join(suggestions) if suggestions else "‚úÖ No obvious issues found"

            print(f"\n[Performance Lab]\n{analysis}\n\n{suggestions_text}\n")
            return (analysis, suggestions_text)

        except json.JSONDecodeError:
            return ("‚ùå Invalid JSON", "Paste valid workflow JSON")
        except Exception as e:
            return (f"‚ùå Error: {e}", "")


class PerfLab_BlackImageFix:
    """Black Image Fix - Diagnose why you're getting dark images."""

    DESCRIPTION = """üîß BLACK IMAGE FIX

HOW TO USE:
1. Select your model type from dropdown
2. Enter your current CFG and steps
3. Read diagnosis and use suggested values

OUTPUTS:
‚Ä¢ diagnosis: What might be wrong
‚Ä¢ suggested_cfg: Optimal CFG for your model
‚Ä¢ suggested_steps: Minimum recommended steps

COMMON CAUSES OF BLACK IMAGES:
‚Ä¢ Flux with CFG > 4 (use 1-3.5)
‚Ä¢ SD with CFG < 3 (use 5-8)
‚Ä¢ Too few steps (< 15)
‚Ä¢ Missing/wrong VAE
‚Ä¢ Empty prompt

FIX IT:
Connect suggested_cfg and suggested_steps
to your KSampler to try the fix!"""

    CATEGORY = "‚ö° Performance Lab/Analysis"
    FUNCTION = "diagnose"
    RETURN_TYPES = ("STRING", "FLOAT", "INT")
    RETURN_NAMES = ("diagnosis", "suggested_cfg", "suggested_steps")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_type": (["SD 1.5", "SDXL", "SD3", "Flux Dev", "Flux Schnell", "Unknown"],),
                "current_cfg": ("FLOAT", {"default": 7.0, "min": 0.0, "max": 30.0}),
                "current_steps": ("INT", {"default": 20, "min": 1, "max": 200}),
            },
        }

    # Optimal settings per model
    MODEL_SETTINGS = {
        "SD 1.5": {"cfg": 7.5, "min_steps": 20, "resolution": 512},
        "SDXL": {"cfg": 7.0, "min_steps": 25, "resolution": 1024},
        "SD3": {"cfg": 4.5, "min_steps": 25, "resolution": 1024},
        "Flux Dev": {"cfg": 3.5, "min_steps": 25, "resolution": 1024},
        "Flux Schnell": {"cfg": 1.0, "min_steps": 4, "resolution": 1024},
        "Unknown": {"cfg": 7.0, "min_steps": 20, "resolution": 768},
    }

    def diagnose(self, model_type: str, current_cfg: float, current_steps: int):
        settings = self.MODEL_SETTINGS.get(model_type, self.MODEL_SETTINGS["Unknown"])
        issues = []

        # Check CFG
        optimal_cfg = settings["cfg"]
        if model_type in ["Flux Dev", "Flux Schnell"] and current_cfg > 4:
            issues.append(f"‚ö†Ô∏è CFG too high for Flux! ({current_cfg} ‚Üí use {optimal_cfg})")
        elif model_type not in ["Flux Dev", "Flux Schnell"] and current_cfg < 3:
            issues.append(f"‚ö†Ô∏è CFG too low for {model_type}! ({current_cfg} ‚Üí use {optimal_cfg})")

        # Check steps
        min_steps = settings["min_steps"]
        if current_steps < min_steps:
            issues.append(f"‚ö†Ô∏è Steps too low! ({current_steps} ‚Üí use at least {min_steps})")

        # General tips
        tips = [
            "",
            "üìã Other common fixes:",
            "‚Ä¢ Add a VAE Loader if using checkpoint's built-in VAE",
            "‚Ä¢ Check all node connections are intact",
            "‚Ä¢ Verify prompt is not empty",
            "‚Ä¢ Try a different sampler (euler, dpmpp_2m)",
        ]

        if issues:
            diagnosis = "\n".join(["üîß Black Image Diagnostic", ""] + issues + tips)
        else:
            diagnosis = f"‚úÖ Settings look OK for {model_type}\n" + "\n".join(tips)

        print(f"\n[Performance Lab]\n{diagnosis}\n")
        return (diagnosis, optimal_cfg, max(current_steps, min_steps))


class PerfLab_Compare:
    """Compare Results - See before/after improvement."""

    DESCRIPTION = """üìä COMPARE RESULTS

HOW TO USE:
1. Run workflow BEFORE optimization, note duration/VRAM
2. Run workflow AFTER optimization
3. Enter both values to see % improvement

INPUTS:
‚Ä¢ before_duration: Time before optimization
‚Ä¢ after_duration: Time after optimization
‚Ä¢ before_vram: Peak VRAM before (optional)
‚Ä¢ after_vram: Peak VRAM after (optional)

OUTPUT:
‚Ä¢ comparison: Table showing changes and % difference

TIPS:
‚Ä¢ Get duration/VRAM from Performance Report node
‚Ä¢ Green = improvement, Red = regression
‚Ä¢ Use to verify optimizations actually helped"""

    CATEGORY = "‚ö° Performance Lab/Analysis"
    FUNCTION = "compare"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("comparison",)
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "before_duration": ("FLOAT", {"default": 0.0}),
                "after_duration": ("FLOAT", {"default": 0.0}),
            },
            "optional": {
                "before_vram": ("FLOAT", {"default": 0.0}),
                "after_vram": ("FLOAT", {"default": 0.0}),
                "before_label": ("STRING", {"default": "Before"}),
                "after_label": ("STRING", {"default": "After"}),
            }
        }

    def compare(self, before_duration, after_duration,
                before_vram=0.0, after_vram=0.0,
                before_label="Before", after_label="After"):

        # Calculate changes
        if before_duration > 0:
            time_change = ((after_duration - before_duration) / before_duration) * 100
            time_str = f"{time_change:+.1f}%"
            time_verdict = "üü¢ Faster!" if time_change < 0 else "üî¥ Slower"
        else:
            time_str = "N/A"
            time_verdict = ""

        if before_vram > 0:
            vram_change = ((after_vram - before_vram) / before_vram) * 100
            vram_str = f"{vram_change:+.1f}%"
            vram_verdict = "üü¢ Less VRAM" if vram_change < 0 else "üî¥ More VRAM"
        else:
            vram_str = "N/A"
            vram_verdict = ""

        comparison = f"""‚ïê‚ïê‚ïê Comparison ‚ïê‚ïê‚ïê

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Metric    ‚îÇ  {before_label:^9}  ‚îÇ  {after_label:^9}  ‚îÇ  Change  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Duration    ‚îÇ  {before_duration:>7.2f}s   ‚îÇ  {after_duration:>7.2f}s   ‚îÇ {time_str:>8} ‚îÇ
‚îÇ Peak VRAM   ‚îÇ  {before_vram:>6.2f} GB  ‚îÇ  {after_vram:>6.2f} GB  ‚îÇ {vram_str:>8} ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

{time_verdict} {vram_verdict}
"""
        print(f"\n[Performance Lab]\n{comparison}")
        return (comparison,)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# LLM INTEGRATION NODES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class PerfLab_GeneratePrompt:
    """Generate LLM Prompt - Get AI help from Claude/GPT/Gemini."""

    DESCRIPTION = """ü§ñ GENERATE LLM PROMPT

HOW TO USE:
1. Enter your optimization goal
2. Add current performance metrics (optional)
3. Copy the output prompt
4. Paste into Claude, ChatGPT, or Gemini
5. Follow the AI's suggestions!

INPUTS:
‚Ä¢ goal: What you want to achieve
‚Ä¢ current_duration: From Performance Report
‚Ä¢ current_vram: From Performance Report
‚Ä¢ model_type: SD 1.5, SDXL, Flux, etc.
‚Ä¢ workflow_json: Paste workflow for analysis

OUTPUT:
‚Ä¢ llm_prompt: Ready to paste into any AI

EXAMPLE GOALS:
‚Ä¢ "Make this run under 10 seconds"
‚Ä¢ "Fit on 8GB VRAM"
‚Ä¢ "Fix why I get black images"
‚Ä¢ "Improve quality without slowing down\""""

    CATEGORY = "‚ö° Performance Lab/LLM"
    FUNCTION = "generate"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("llm_prompt",)
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "goal": ("STRING", {
                    "multiline": True,
                    "default": "Make this workflow faster and use less VRAM"
                }),
            },
            "optional": {
                "current_duration": ("FLOAT", {"default": 0.0}),
                "current_vram": ("FLOAT", {"default": 0.0}),
                "model_type": (["SD 1.5", "SDXL", "SD3", "Flux", "Unknown"],),
                "workflow_json": ("STRING", {"multiline": True, "default": ""}),
            }
        }

    def generate(self, goal, current_duration=0.0, current_vram=0.0,
                 model_type="Unknown", workflow_json=""):

        prompt_parts = [
            "I need help optimizing my ComfyUI workflow.",
            "",
            f"**My Goal:** {goal}",
            "",
        ]

        if current_duration > 0:
            prompt_parts.append(f"**Current Duration:** {current_duration:.2f} seconds")
        if current_vram > 0:
            prompt_parts.append(f"**Current VRAM Usage:** {current_vram:.2f} GB")
        if model_type != "Unknown":
            prompt_parts.append(f"**Model Type:** {model_type}")

        prompt_parts.extend([
            "",
            "Please suggest specific changes I can make to improve performance.",
            "Focus on:",
            "- Resolution adjustments",
            "- Step count optimization",
            "- Sampler selection",
            "- CFG scale",
            "- Any nodes I could bypass or optimize",
            "",
            "Provide specific values I should use.",
        ])

        if workflow_json and workflow_json.strip().startswith("{"):
            prompt_parts.extend([
                "",
                "**Workflow JSON:**",
                "```json",
                workflow_json[:2000] + ("..." if len(workflow_json) > 2000 else ""),
                "```"
            ])

        prompt = "\n".join(prompt_parts)
        print(f"\n[Performance Lab] LLM Prompt Generated ({len(prompt)} chars)\n")
        print("Copy this prompt and paste it into Claude, ChatGPT, or Gemini:\n")
        print("-" * 60)
        print(prompt[:500] + "..." if len(prompt) > 500 else prompt)
        print("-" * 60)

        return (prompt,)


class PerfLab_LLMClient:
    """LLM Client - Call Ollama, OpenAI, or compatible APIs directly."""

    DESCRIPTION = """ü§ñ LLM CLIENT - CALL AI DIRECTLY

HOW TO USE:
1. Select your LLM provider (Ollama is free & local!)
2. Enter your prompt or connect from Generate Prompt
3. Get AI response directly in ComfyUI

PROVIDERS:
‚Ä¢ Ollama (FREE, LOCAL): Install from ollama.com, run locally
‚Ä¢ OpenAI: Requires API key
‚Ä¢ Anthropic: Requires API key
‚Ä¢ Custom: Any OpenAI-compatible API

DEFAULT (Ollama):
‚Ä¢ URL: http://127.0.0.1:11434
‚Ä¢ Model: llama3.2 (or mistral, codellama, etc.)

OUTPUTS:
‚Ä¢ response: The LLM's response text
‚Ä¢ success: Whether the call succeeded"""

    CATEGORY = "‚ö° Performance Lab/LLM"
    FUNCTION = "call_llm"
    RETURN_TYPES = ("STRING", "BOOLEAN", "STRING")
    RETURN_NAMES = ("response", "success", "status")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "Analyze this ComfyUI workflow and suggest optimizations.",
                    "tooltip": "The prompt to send to the LLM"
                }),
                "provider": (["Ollama (Local)", "OpenAI", "Anthropic", "Custom"],
                            {"default": "Ollama (Local)",
                             "tooltip": "Ollama is FREE and runs locally!"}),
            },
            "optional": {
                "api_url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "tooltip": "API URL (Ollama: 11434, OpenAI: api.openai.com)"
                }),
                "model": ("STRING", {
                    "default": "llama3.2",
                    "tooltip": "Model name (Ollama: llama3.2, mistral, codellama)"
                }),
                "api_key": ("STRING", {
                    "default": "",
                    "tooltip": "API key (not needed for Ollama)"
                }),
                "system_prompt": ("STRING", {
                    "multiline": True,
                    "default": "You are a ComfyUI workflow optimization expert. Provide specific, actionable suggestions.",
                    "tooltip": "System prompt to set AI behavior"
                }),
                "max_tokens": ("INT", {
                    "default": 2048,
                    "min": 100,
                    "max": 8192,
                    "tooltip": "Maximum response length"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "Creativity (0=focused, 1+=creative)"
                }),
            }
        }

    def call_llm(self, prompt, provider, api_url="http://127.0.0.1:11434",
                 model="llama3.2", api_key="", system_prompt="",
                 max_tokens=2048, temperature=0.7):

        import urllib.request
        import urllib.error

        try:
            if provider == "Ollama (Local)":
                return self._call_ollama(api_url, model, prompt, system_prompt, max_tokens, temperature)
            elif provider == "OpenAI":
                return self._call_openai(api_key, model, prompt, system_prompt, max_tokens, temperature)
            elif provider == "Anthropic":
                return self._call_anthropic(api_key, model, prompt, system_prompt, max_tokens, temperature)
            else:  # Custom - use OpenAI format
                return self._call_openai_format(api_url, api_key, model, prompt, system_prompt, max_tokens, temperature)
        except Exception as e:
            error_msg = f"‚ùå LLM Error: {str(e)}"
            print(f"[Performance Lab] {error_msg}")
            return ("", False, error_msg)

    def _call_ollama(self, api_url, model, prompt, system_prompt, max_tokens, temperature):
        """Call Ollama API (local, free)."""
        import urllib.request

        url = f"{api_url.rstrip('/')}/api/generate"

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": max_tokens,
                "temperature": temperature,
            }
        }

        if system_prompt:
            payload["system"] = system_prompt

        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(url, data=data, headers={"Content-Type": "application/json"})

        try:
            with urllib.request.urlopen(req, timeout=120) as response:
                result = json.loads(response.read().decode('utf-8'))
                text = result.get("response", "")
                print(f"[Performance Lab] Ollama response received ({len(text)} chars)")
                return (text, True, f"‚úÖ Ollama ({model}): {len(text)} chars")
        except urllib.error.URLError as e:
            return ("", False, f"‚ùå Cannot connect to Ollama at {api_url}. Is it running?")

    def _call_openai(self, api_key, model, prompt, system_prompt, max_tokens, temperature):
        """Call OpenAI API."""
        return self._call_openai_format(
            "https://api.openai.com/v1", api_key,
            model or "gpt-4o-mini", prompt, system_prompt, max_tokens, temperature
        )

    def _call_anthropic(self, api_key, model, prompt, system_prompt, max_tokens, temperature):
        """Call Anthropic API."""
        import urllib.request

        url = "https://api.anthropic.com/v1/messages"

        messages = [{"role": "user", "content": prompt}]

        payload = {
            "model": model or "claude-3-haiku-20240307",
            "max_tokens": max_tokens,
            "messages": messages,
        }

        if system_prompt:
            payload["system"] = system_prompt

        headers = {
            "Content-Type": "application/json",
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01"
        }

        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(url, data=data, headers=headers)

        with urllib.request.urlopen(req, timeout=120) as response:
            result = json.loads(response.read().decode('utf-8'))
            text = result.get("content", [{}])[0].get("text", "")
            return (text, True, f"‚úÖ Anthropic ({model}): {len(text)} chars")

    def _call_openai_format(self, api_url, api_key, model, prompt, system_prompt, max_tokens, temperature):
        """Call OpenAI-compatible API."""
        import urllib.request

        url = f"{api_url.rstrip('/')}/chat/completions"

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }

        headers = {"Content-Type": "application/json"}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"

        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(url, data=data, headers=headers)

        with urllib.request.urlopen(req, timeout=120) as response:
            result = json.loads(response.read().decode('utf-8'))
            text = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            return (text, True, f"‚úÖ {model}: {len(text)} chars")


class PerfLab_AutoOptimize:
    """Auto Optimize - Send performance data to LLM and get optimization suggestions."""

    DESCRIPTION = """üöÄ AUTO OPTIMIZE - AI-POWERED SUGGESTIONS

THE AUTOMATED OPTIMIZATION LOOP!

HOW TO USE:
1. Connect your performance report
2. Select optimization goal (speed/vram/quality)
3. Run to get AI suggestions
4. Apply suggestions and test
5. Keep or reject based on results

INPUTS:
‚Ä¢ performance_report: From Performance Report node
‚Ä¢ goal: What to optimize for
‚Ä¢ current values: Your current settings

OUTPUTS:
‚Ä¢ suggested_steps: Recommended step count
‚Ä¢ suggested_resolution: Recommended resolution
‚Ä¢ suggested_cfg: Recommended CFG
‚Ä¢ explanation: Why these changes help
‚Ä¢ raw_response: Full LLM response

WORKFLOW:
Run ‚Üí Get Suggestions ‚Üí Apply ‚Üí Test ‚Üí Keep/Reject ‚Üí Repeat!"""

    CATEGORY = "‚ö° Performance Lab/LLM"
    FUNCTION = "optimize"
    RETURN_TYPES = ("INT", "INT", "FLOAT", "STRING", "STRING")
    RETURN_NAMES = ("suggested_steps", "suggested_resolution", "suggested_cfg", "explanation", "raw_response")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "goal": (["speed", "vram", "quality", "balanced"],
                        {"default": "speed",
                         "tooltip": "What to optimize for"}),
            },
            "optional": {
                "performance_report": ("STRING", {
                    "forceInput": True,
                    "tooltip": "Connect from Performance Report node"
                }),
                "current_steps": ("INT", {
                    "default": 30,
                    "min": 1,
                    "max": 150,
                    "tooltip": "Your current step count"
                }),
                "current_resolution": ("INT", {
                    "default": 1024,
                    "min": 256,
                    "max": 4096,
                    "tooltip": "Your current resolution"
                }),
                "current_cfg": ("FLOAT", {
                    "default": 7.0,
                    "min": 0.0,
                    "max": 30.0,
                    "tooltip": "Your current CFG"
                }),
                "model_type": (["SD 1.5", "SDXL", "SD3", "Flux Dev", "Flux Schnell", "Unknown"],
                              {"default": "Unknown",
                               "tooltip": "Your model type (affects recommendations)"}),
                "ollama_url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "tooltip": "Ollama API URL"
                }),
                "ollama_model": ("STRING", {
                    "default": "llama3.2",
                    "tooltip": "Ollama model to use"
                }),
            }
        }

    def optimize(self, goal, performance_report="", current_steps=30,
                 current_resolution=1024, current_cfg=7.0, model_type="Unknown",
                 ollama_url="http://127.0.0.1:11434", ollama_model="llama3.2"):

        # Build the optimization prompt
        prompt = f"""Analyze this ComfyUI workflow performance and suggest optimizations.

GOAL: {goal.upper()}

CURRENT SETTINGS:
- Steps: {current_steps}
- Resolution: {current_resolution}
- CFG: {current_cfg}
- Model Type: {model_type}

PERFORMANCE DATA:
{performance_report if performance_report else "No performance data yet"}

Based on the goal of "{goal}", suggest SPECIFIC numeric values for:
1. Steps (integer between 4-100)
2. Resolution (integer, multiple of 8, between 256-2048)
3. CFG (float between 0.5-20)

IMPORTANT: Respond in this EXACT format:
STEPS: [number]
RESOLUTION: [number]
CFG: [number]
EXPLANATION: [brief explanation why]

Example:
STEPS: 20
RESOLUTION: 768
CFG: 7.0
EXPLANATION: Reducing steps from 30 to 20 saves ~33% time with minimal quality loss. 768px is optimal for 8GB VRAM.
"""

        system_prompt = """You are a ComfyUI optimization expert. Given performance metrics and a goal, suggest specific numeric parameter changes.

Key optimization rules:
- For SPEED: Reduce steps (15-25 usually enough), lower resolution
- For VRAM: Reduce resolution first (has quadratic effect), then batch size, use tiled VAE
- For QUALITY: More steps (30-50), native resolution, appropriate CFG for model
- Flux models need CFG 1-4, SD models need CFG 5-8
- SD1.5 native: 512px, SDXL/Flux native: 1024px

Always respond with EXACT numbers in the format requested."""

        # Call Ollama
        try:
            import urllib.request

            url = f"{ollama_url.rstrip('/')}/api/generate"
            payload = {
                "model": ollama_model,
                "prompt": prompt,
                "system": system_prompt,
                "stream": False,
                "options": {"temperature": 0.3, "num_predict": 500}
            }

            data = json.dumps(payload).encode('utf-8')
            req = urllib.request.Request(url, data=data, headers={"Content-Type": "application/json"})

            with urllib.request.urlopen(req, timeout=60) as response:
                result = json.loads(response.read().decode('utf-8'))
                llm_response = result.get("response", "")

            # Parse the response
            suggested_steps = current_steps
            suggested_resolution = current_resolution
            suggested_cfg = current_cfg
            explanation = "Could not parse LLM response"

            lines = llm_response.upper().split('\n')
            for line in lines:
                if line.startswith('STEPS:'):
                    try:
                        suggested_steps = int(''.join(filter(str.isdigit, line.split(':')[1])))
                    except:
                        pass
                elif line.startswith('RESOLUTION:'):
                    try:
                        suggested_resolution = int(''.join(filter(str.isdigit, line.split(':')[1])))
                    except:
                        pass
                elif line.startswith('CFG:'):
                    try:
                        cfg_str = line.split(':')[1].strip()
                        suggested_cfg = float(''.join(c for c in cfg_str if c.isdigit() or c == '.'))
                    except:
                        pass

            # Extract explanation
            if 'EXPLANATION:' in llm_response.upper():
                explanation = llm_response.split('EXPLANATION:')[-1].split('xplanation:')[-1].strip()

            # Clamp values to safe ranges
            suggested_steps = max(4, min(100, suggested_steps))
            suggested_resolution = max(256, min(2048, (suggested_resolution // 8) * 8))
            suggested_cfg = max(0.5, min(20.0, suggested_cfg))

            print(f"[Performance Lab] AutoOptimize suggestions:")
            print(f"  Steps: {current_steps} ‚Üí {suggested_steps}")
            print(f"  Resolution: {current_resolution} ‚Üí {suggested_resolution}")
            print(f"  CFG: {current_cfg} ‚Üí {suggested_cfg}")

            return (suggested_steps, suggested_resolution, suggested_cfg, explanation, llm_response)

        except urllib.error.URLError:
            explanation = "‚ùå Cannot connect to Ollama. Make sure it's running (ollama serve)"
            return (current_steps, current_resolution, current_cfg, explanation, "Connection failed")
        except Exception as e:
            explanation = f"‚ùå Error: {str(e)}"
            return (current_steps, current_resolution, current_cfg, explanation, str(e))


class PerfLab_ApplyOrRevert:
    """Apply or Revert - Compare results and decide to keep or reject changes."""

    DESCRIPTION = """‚úÖ‚ùå APPLY OR REVERT

COMPARE BEFORE/AFTER AND DECIDE!

HOW TO USE:
1. Connect your BEFORE results (original)
2. Connect your AFTER results (with optimization)
3. Set keep_changes based on results
4. Output goes to your workflow

This is the DECISION node in the optimization loop:
‚Ä¢ If AFTER is better ‚Üí keep_changes = TRUE ‚Üí use new values
‚Ä¢ If AFTER is worse ‚Üí keep_changes = FALSE ‚Üí use original values

OUTPUTS:
‚Ä¢ The values based on your decision
‚Ä¢ comparison: Shows the improvement/regression"""

    CATEGORY = "‚ö° Performance Lab/LLM"
    FUNCTION = "decide"
    RETURN_TYPES = ("INT", "INT", "FLOAT", "STRING")
    RETURN_NAMES = ("steps", "resolution", "cfg", "comparison")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "keep_changes": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "TRUE = use new values, FALSE = use original"
                }),
                "original_steps": ("INT", {"default": 30, "min": 1, "max": 150}),
                "original_resolution": ("INT", {"default": 1024, "min": 256, "max": 4096}),
                "original_cfg": ("FLOAT", {"default": 7.0, "min": 0.0, "max": 30.0}),
                "new_steps": ("INT", {"default": 20, "min": 1, "max": 150}),
                "new_resolution": ("INT", {"default": 768, "min": 256, "max": 4096}),
                "new_cfg": ("FLOAT", {"default": 7.0, "min": 0.0, "max": 30.0}),
            },
            "optional": {
                "before_duration": ("FLOAT", {"default": 0.0, "tooltip": "Duration before optimization"}),
                "after_duration": ("FLOAT", {"default": 0.0, "tooltip": "Duration after optimization"}),
                "before_vram": ("FLOAT", {"default": 0.0, "tooltip": "VRAM before"}),
                "after_vram": ("FLOAT", {"default": 0.0, "tooltip": "VRAM after"}),
            }
        }

    def decide(self, keep_changes, original_steps, original_resolution, original_cfg,
               new_steps, new_resolution, new_cfg,
               before_duration=0.0, after_duration=0.0, before_vram=0.0, after_vram=0.0):

        # Calculate changes
        if before_duration > 0 and after_duration > 0:
            speed_change = ((after_duration - before_duration) / before_duration) * 100
            speed_str = f"{speed_change:+.1f}%"
            speed_verdict = "üü¢ Faster!" if speed_change < 0 else "üî¥ Slower"
        else:
            speed_str = "N/A"
            speed_verdict = ""

        if before_vram > 0 and after_vram > 0:
            vram_change = ((after_vram - before_vram) / before_vram) * 100
            vram_str = f"{vram_change:+.1f}%"
            vram_verdict = "üü¢ Less VRAM" if vram_change < 0 else "üî¥ More VRAM"
        else:
            vram_str = "N/A"
            vram_verdict = ""

        decision = "‚úÖ KEEPING NEW VALUES" if keep_changes else "‚ùå REVERTING TO ORIGINAL"

        comparison = f"""‚ïê‚ïê‚ïê Optimization Decision ‚ïê‚ïê‚ïê

{decision}

PERFORMANCE COMPARISON:
‚Ä¢ Duration: {before_duration:.2f}s ‚Üí {after_duration:.2f}s ({speed_str}) {speed_verdict}
‚Ä¢ VRAM: {before_vram:.2f}GB ‚Üí {after_vram:.2f}GB ({vram_str}) {vram_verdict}

SETTINGS:
‚Ä¢ Steps: {original_steps} ‚Üí {new_steps}
‚Ä¢ Resolution: {original_resolution} ‚Üí {new_resolution}
‚Ä¢ CFG: {original_cfg} ‚Üí {new_cfg}

OUTPUT: {'New values' if keep_changes else 'Original values'}"""

        print(f"[Performance Lab] {decision}")

        if keep_changes:
            return (new_steps, new_resolution, new_cfg, comparison)
        else:
            return (original_steps, original_resolution, original_cfg, comparison)


class PerfLab_ShowText:
    """Show Text - Display any text in the node preview."""

    DESCRIPTION = """üìù SHOW TEXT

HOW TO USE:
1. Connect any STRING output to this node
2. The text appears in both the node and console

USE FOR:
‚Ä¢ Viewing analysis results
‚Ä¢ Checking LLM prompts before copying
‚Ä¢ Debugging string outputs
‚Ä¢ Displaying reports

TIPS:
‚Ä¢ Great for inspecting what's in a string
‚Ä¢ Connect to Performance Report output
‚Ä¢ Connect to Analyzer suggestions output"""

    CATEGORY = "‚ö° Performance Lab/Utility"
    FUNCTION = "show"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {"forceInput": True}),
            }
        }

    def show(self, text):
        print(f"\n[Performance Lab]\n{text}\n")
        return (text,)


class PerfLab_Switch:
    """A/B Switch - Toggle between two any-type inputs."""

    DESCRIPTION = """üîÄ A/B SWITCH

HOW TO USE:
1. Connect your TEST value to 'a_test'
2. Connect your PRODUCTION value to 'b_production'
3. Toggle 'use_b' to switch between them

INPUTS:
‚Ä¢ a_test: Your optimized/test configuration
‚Ä¢ b_production: Your original/production configuration
‚Ä¢ use_b: Toggle (OFF = test, ON = production)

USE FOR:
‚Ä¢ Comparing before/after settings
‚Ä¢ Quick test mode vs production mode toggle
‚Ä¢ Switching between different node chains
‚Ä¢ A/B testing different approaches

TIPS:
‚Ä¢ Works with ANY type (images, latents, models, etc.)
‚Ä¢ Use with Int/Float Switch for typed values"""

    CATEGORY = "‚ö° Performance Lab/Utility"
    FUNCTION = "switch"
    RETURN_TYPES = ("*",)
    RETURN_NAMES = ("output",)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "use_b": ("BOOLEAN", {"default": False,
                         "tooltip": "OFF = use A (test), ON = use B (production)"}),
            },
            "optional": {
                "a_test": ("*",),
                "b_production": ("*",),
            }
        }

    def switch(self, use_b, a_test=None, b_production=None):
        if use_b:
            return (b_production,)
        return (a_test,)


class PerfLab_IntSwitch:
    """Integer A/B Switch - Toggle between two integer values."""

    DESCRIPTION = """üî¢ INTEGER A/B SWITCH

HOW TO USE:
1. Enter your TEST value in 'a_test' (e.g., 15 steps)
2. Enter your PRODUCTION value in 'b_production' (e.g., 30 steps)
3. Toggle 'use_b' to switch between them

INPUTS:
‚Ä¢ a_test: Test/fast integer value
‚Ä¢ b_production: Production/quality integer value
‚Ä¢ use_b: Toggle (OFF = test, ON = production)

USE FOR:
‚Ä¢ Switching step counts (15 vs 30)
‚Ä¢ Switching resolutions (512 vs 1024)
‚Ä¢ Switching batch sizes (1 vs 4)
‚Ä¢ Any integer A/B comparison

EXAMPLE:
‚Ä¢ a_test: 15 (fast testing)
‚Ä¢ b_production: 30 (final quality)
‚Ä¢ Connect output to KSampler steps"""

    CATEGORY = "‚ö° Performance Lab/Utility"
    FUNCTION = "switch"
    RETURN_TYPES = ("INT",)
    RETURN_NAMES = ("value",)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "a_test": ("INT", {"default": 15, "min": 0, "max": 10000}),
                "b_production": ("INT", {"default": 30, "min": 0, "max": 10000}),
                "use_b": ("BOOLEAN", {"default": False}),
            }
        }

    def switch(self, a_test, b_production, use_b):
        return (b_production if use_b else a_test,)


class PerfLab_FloatSwitch:
    """Float A/B Switch - Toggle between two decimal values."""

    DESCRIPTION = """üî¢ FLOAT A/B SWITCH

HOW TO USE:
1. Enter your TEST value in 'a_test' (e.g., 1.0 CFG)
2. Enter your PRODUCTION value in 'b_production' (e.g., 7.0 CFG)
3. Toggle 'use_b' to switch between them

INPUTS:
‚Ä¢ a_test: Test float value
‚Ä¢ b_production: Production float value
‚Ä¢ use_b: Toggle (OFF = test, ON = production)

USE FOR:
‚Ä¢ Switching CFG scale (1.0 vs 7.0)
‚Ä¢ Switching denoise strength (0.5 vs 1.0)
‚Ä¢ Any decimal A/B comparison

EXAMPLE (Flux vs SD):
‚Ä¢ a_test: 1.0 (Flux CFG)
‚Ä¢ b_production: 7.0 (SD CFG)
‚Ä¢ Toggle based on which model you're using"""

    CATEGORY = "‚ö° Performance Lab/Utility"
    FUNCTION = "switch"
    RETURN_TYPES = ("FLOAT",)
    RETURN_NAMES = ("value",)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "a_test": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step": 0.1}),
                "b_production": ("FLOAT", {"default": 7.0, "min": 0.0, "max": 100.0, "step": 0.1}),
                "use_b": ("BOOLEAN", {"default": False}),
            }
        }

    def switch(self, a_test, b_production, use_b):
        return (b_production if use_b else a_test,)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# META-WORKFLOW NODES (Run & Test Other Workflows)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class PerfLab_LoadWorkflow:
    """Load Workflow - Load a workflow JSON file for analysis or execution."""

    DESCRIPTION = """üìÇ LOAD WORKFLOW

HOW TO USE:
1. Select a workflow from dropdown OR enter custom path
2. The workflow is loaded and output as JSON string
3. Connect to Analyzer or Queue Workflow node

INPUTS:
‚Ä¢ workflow_folder: Select from common ComfyUI folders
‚Ä¢ file_name: Select workflow or enter filename
‚Ä¢ custom_path: (Optional) Full path for files elsewhere

OUTPUTS:
‚Ä¢ workflow_json: The loaded workflow as JSON string
‚Ä¢ node_count: Number of nodes in the workflow
‚Ä¢ status: Success/error message

USE FOR:
‚Ä¢ Loading workflows for analysis
‚Ä¢ Batch testing multiple workflows
‚Ä¢ Building a workflow test suite

TIPS:
‚Ä¢ Leave custom_path empty to use folder selection
‚Ä¢ Workflows are auto-discovered from ComfyUI folders
‚Ä¢ Connect to Queue Workflow to run it"""

    CATEGORY = "‚ö° Performance Lab/Meta-Workflow"
    FUNCTION = "load"
    RETURN_TYPES = ("STRING", "INT", "STRING")
    RETURN_NAMES = ("workflow_json", "node_count", "status")

    # Common workflow locations relative to ComfyUI
    WORKFLOW_FOLDERS = [
        "user/default/workflows",
        "output",
        "input",
        "custom_nodes/ComfyUI_PerformanceLab/examples",
    ]

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "workflow_folder": (["user/default/workflows", "output", "input", "examples", "custom_path"],
                                   {"default": "user/default/workflows",
                                    "tooltip": "Select folder or use 'custom_path' for manual entry"}),
                "file_name": ("STRING", {
                    "default": "workflow.json",
                    "tooltip": "Workflow filename (with .json extension)"
                }),
            },
            "optional": {
                "custom_path": ("STRING", {
                    "default": "",
                    "tooltip": "Full path (only used when folder is set to 'custom_path')"
                }),
            }
        }

    def load(self, workflow_folder: str, file_name: str, custom_path: str = ""):
        # Determine the file path
        if workflow_folder == "custom_path":
            if not custom_path:
                return ("", 0, "‚ùå custom_path selected but no path provided")
            file_path = custom_path
        else:
            # Build path relative to ComfyUI installation
            # Try to find ComfyUI root
            comfy_root = os.path.dirname(os.path.dirname(os.path.dirname(MODULE_DIR)))

            if workflow_folder == "examples":
                # Our examples folder
                file_path = os.path.join(MODULE_DIR, "examples", file_name)
            else:
                file_path = os.path.join(comfy_root, workflow_folder, file_name)

        if not file_path:
            return ("", 0, "‚ùå No file path provided")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            workflow = json.loads(content)
            nodes = workflow.get("nodes", [])
            node_count = len(nodes)

            print(f"[Performance Lab] Loaded workflow: {file_path}")
            print(f"   Nodes: {node_count}")

            return (content, node_count, f"‚úÖ Loaded {node_count} nodes from {os.path.basename(file_path)}")

        except FileNotFoundError:
            return ("", 0, f"‚ùå File not found: {file_path}")
        except json.JSONDecodeError as e:
            return ("", 0, f"‚ùå Invalid JSON: {e}")
        except Exception as e:
            return ("", 0, f"‚ùå Error: {e}")


class PerfLab_QueueWorkflow:
    """Queue Workflow - Send a workflow to ComfyUI for execution."""

    DESCRIPTION = """‚ñ∂Ô∏è QUEUE WORKFLOW

HOW TO USE:
1. Connect workflow_json from Load Workflow node
2. Set your ComfyUI server URL (default: localhost:8188)
3. Toggle 'execute' to True to run
4. The workflow will be queued on ComfyUI

INPUTS:
‚Ä¢ workflow_json: The workflow JSON to execute
‚Ä¢ server_url: ComfyUI server address
‚Ä¢ execute: Safety toggle - must be True to run
‚Ä¢ client_id: Optional ID for tracking

OUTPUTS:
‚Ä¢ prompt_id: The queued prompt ID
‚Ä¢ status: Success/error message

USE FOR:
‚Ä¢ Running test workflows automatically
‚Ä¢ Batch testing multiple workflows
‚Ä¢ Remote workflow execution
‚Ä¢ Performance benchmarking

TIPS:
‚Ä¢ Make sure ComfyUI is running first
‚Ä¢ The 'execute' toggle prevents accidents
‚Ä¢ Works with remote ComfyUI servers too"""

    CATEGORY = "‚ö° Performance Lab/Meta-Workflow"
    FUNCTION = "queue"
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("prompt_id", "status")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "workflow_json": ("STRING", {"forceInput": True}),
                "server_url": ("STRING", {"default": "http://127.0.0.1:8188"}),
                "execute": ("BOOLEAN", {"default": False,
                           "tooltip": "Toggle ON to actually queue the workflow"}),
            },
            "optional": {
                "client_id": ("STRING", {"default": "perflab"}),
            }
        }

    def queue(self, workflow_json: str, server_url: str, execute: bool, client_id: str = "perflab"):
        if not execute:
            return ("", "‚è∏Ô∏è Execute is OFF - toggle to True to run")

        if not workflow_json:
            return ("", "‚ùå No workflow provided")

        try:
            import urllib.request
            import urllib.error

            # Parse workflow
            workflow = json.loads(workflow_json)

            # Prepare prompt payload
            prompt_payload = {
                "prompt": workflow,
                "client_id": client_id
            }

            # Send to ComfyUI
            url = f"{server_url.rstrip('/')}/prompt"
            data = json.dumps(prompt_payload).encode('utf-8')

            req = urllib.request.Request(
                url,
                data=data,
                headers={"Content-Type": "application/json"}
            )

            with urllib.request.urlopen(req, timeout=10) as response:
                result = json.loads(response.read().decode('utf-8'))
                prompt_id = result.get("prompt_id", "unknown")

                print(f"[Performance Lab] Workflow queued!")
                print(f"   Prompt ID: {prompt_id}")
                print(f"   Server: {server_url}")

                return (prompt_id, f"‚úÖ Queued: {prompt_id}")

        except urllib.error.URLError as e:
            return ("", f"‚ùå Connection failed: {e}")
        except json.JSONDecodeError:
            return ("", "‚ùå Invalid workflow JSON")
        except Exception as e:
            return ("", f"‚ùå Error: {e}")


class PerfLab_EndpointHealth:
    """Endpoint Health Check - Check if a network service is running."""

    DESCRIPTION = """üè• ENDPOINT HEALTH CHECK

HOW TO USE:
1. Enter the URL of your service (e.g., http://localhost:7860)
2. Select the service type for correct health check
3. Run to check if it's online and responding

INPUTS:
‚Ä¢ url: Full URL to the service
‚Ä¢ service_type: Type of service (ComfyUI, Automatic1111, Ollama, etc.)
‚Ä¢ timeout: How long to wait (seconds)

OUTPUTS:
‚Ä¢ is_healthy: Boolean - True if service is up
‚Ä¢ latency_ms: Response time in milliseconds
‚Ä¢ status: Detailed status message

SUPPORTED SERVICES:
‚Ä¢ ComfyUI (port 8188)
‚Ä¢ Automatic1111/Forge (port 7860)
‚Ä¢ Ollama (port 11434)
‚Ä¢ KoboldCpp (port 5001)
‚Ä¢ Custom (any HTTP endpoint)

USE FOR:
‚Ä¢ Checking if services are online before running
‚Ä¢ Measuring network latency
‚Ä¢ Distributed workflow health monitoring"""

    CATEGORY = "‚ö° Performance Lab/Network"
    FUNCTION = "check"
    RETURN_TYPES = ("BOOLEAN", "FLOAT", "STRING")
    RETURN_NAMES = ("is_healthy", "latency_ms", "status")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "url": ("STRING", {"default": "http://127.0.0.1:8188"}),
                "service_type": (["ComfyUI", "Automatic1111", "Ollama", "KoboldCpp", "Whisper", "TTS", "Custom"],),
            },
            "optional": {
                "timeout": ("FLOAT", {"default": 5.0, "min": 1.0, "max": 30.0}),
            }
        }

    # Health check endpoints per service type
    HEALTH_ENDPOINTS = {
        "ComfyUI": "/system_stats",
        "Automatic1111": "/sdapi/v1/options",
        "Ollama": "/api/tags",
        "KoboldCpp": "/api/v1/info/version",
        "Whisper": "/",
        "TTS": "/",
        "Custom": "/",
    }

    def check(self, url: str, service_type: str, timeout: float = 5.0):
        import urllib.request
        import urllib.error

        endpoint = self.HEALTH_ENDPOINTS.get(service_type, "/")
        full_url = f"{url.rstrip('/')}{endpoint}"

        start_time = time.time()

        try:
            req = urllib.request.Request(full_url, method='GET')
            with urllib.request.urlopen(req, timeout=timeout) as response:
                latency = (time.time() - start_time) * 1000  # ms
                status_code = response.status

                status = f"‚úÖ {service_type} online ({status_code}) - {latency:.1f}ms"
                print(f"[Performance Lab] {status}")
                return (True, latency, status)

        except urllib.error.HTTPError as e:
            latency = (time.time() - start_time) * 1000
            # Some services return errors but are still "up"
            if e.code in [401, 403, 404, 405]:
                status = f"‚ö†Ô∏è {service_type} responding ({e.code}) - {latency:.1f}ms"
                print(f"[Performance Lab] {status}")
                return (True, latency, status)
            return (False, latency, f"‚ùå HTTP {e.code}: {e.reason}")

        except urllib.error.URLError as e:
            return (False, 0.0, f"‚ùå Cannot connect: {e.reason}")
        except Exception as e:
            return (False, 0.0, f"‚ùå Error: {e}")


class PerfLab_NetworkScanner:
    """Network Scanner - Find generative AI services on your network."""

    DESCRIPTION = """üîç NETWORK SCANNER

HOW TO USE:
1. Enter the base IP (e.g., 192.168.1) or localhost
2. Select which services to scan for
3. Run to discover running services

INPUTS:
‚Ä¢ base_ip: Network prefix or 'localhost' for local only
‚Ä¢ scan_comfyui: Check for ComfyUI instances
‚Ä¢ scan_a1111: Check for Automatic1111/Forge
‚Ä¢ scan_ollama: Check for Ollama LLM server
‚Ä¢ scan_kobold: Check for KoboldCpp

OUTPUTS:
‚Ä¢ found_services: JSON list of discovered services
‚Ä¢ count: Number of services found
‚Ä¢ report: Human-readable summary

USE FOR:
‚Ä¢ Discovering AI services on your network
‚Ä¢ Building distributed pipeline configurations
‚Ä¢ Finding available compute resources

TIPS:
‚Ä¢ Localhost scan is fast (checks common ports)
‚Ä¢ Network scan checks .1-.254 (takes longer)
‚Ä¢ Only scans standard ports per service"""

    CATEGORY = "‚ö° Performance Lab/Network"
    FUNCTION = "scan"
    RETURN_TYPES = ("STRING", "INT", "STRING")
    RETURN_NAMES = ("found_services", "count", "report")
    OUTPUT_NODE = True

    # Standard ports per service
    SERVICE_PORTS = {
        "ComfyUI": [8188, 8189],
        "Automatic1111": [7860, 7861],
        "Ollama": [11434],
        "KoboldCpp": [5001, 5000],
    }

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "base_ip": ("STRING", {"default": "localhost"}),
            },
            "optional": {
                "scan_comfyui": ("BOOLEAN", {"default": True}),
                "scan_a1111": ("BOOLEAN", {"default": True}),
                "scan_ollama": ("BOOLEAN", {"default": True}),
                "scan_kobold": ("BOOLEAN", {"default": True}),
            }
        }

    def scan(self, base_ip: str, scan_comfyui=True, scan_a1111=True,
             scan_ollama=True, scan_kobold=True):
        import socket

        found = []
        services_to_scan = {}

        if scan_comfyui:
            services_to_scan["ComfyUI"] = self.SERVICE_PORTS["ComfyUI"]
        if scan_a1111:
            services_to_scan["Automatic1111"] = self.SERVICE_PORTS["Automatic1111"]
        if scan_ollama:
            services_to_scan["Ollama"] = self.SERVICE_PORTS["Ollama"]
        if scan_kobold:
            services_to_scan["KoboldCpp"] = self.SERVICE_PORTS["KoboldCpp"]

        print(f"[Performance Lab] Scanning for services on {base_ip}...")

        # Determine hosts to scan
        if base_ip.lower() in ["localhost", "127.0.0.1", "local"]:
            hosts = ["127.0.0.1"]
        else:
            # Scan local subnet
            hosts = [f"{base_ip}.{i}" for i in range(1, 255)]

        for host in hosts:
            for service_name, ports in services_to_scan.items():
                for port in ports:
                    try:
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        sock.settimeout(0.5 if host == "127.0.0.1" else 0.2)
                        result = sock.connect_ex((host, port))
                        sock.close()

                        if result == 0:
                            entry = {
                                "service": service_name,
                                "host": host,
                                "port": port,
                                "url": f"http://{host}:{port}"
                            }
                            found.append(entry)
                            print(f"   ‚úÖ Found {service_name} at {host}:{port}")

                    except:
                        pass

        # Build report
        if found:
            report_lines = ["‚ïê‚ïê‚ïê Services Found ‚ïê‚ïê‚ïê", ""]
            for svc in found:
                report_lines.append(f"‚Ä¢ {svc['service']}: {svc['url']}")
            report = "\n".join(report_lines)
        else:
            report = "‚ùå No services found"

        print(f"[Performance Lab] Scan complete: {len(found)} services found")

        return (json.dumps(found, indent=2), len(found), report)


class PerfLab_BenchmarkRunner:
    """Benchmark Runner - Run a workflow multiple times and average results."""

    DESCRIPTION = """üèÅ BENCHMARK RUNNER

HOW TO USE:
1. Connect workflow_json from Load Workflow
2. Set number of runs (3-5 recommended)
3. Connect to Queue Workflow for execution
4. View averaged performance metrics

INPUTS:
‚Ä¢ runs: Number of times to run (1-10)
‚Ä¢ warmup_runs: Discard first N runs (0-3)
‚Ä¢ delay_between: Seconds between runs

OUTPUTS:
‚Ä¢ avg_duration: Average time per run
‚Ä¢ min_duration: Fastest run
‚Ä¢ max_duration: Slowest run
‚Ä¢ report: Full benchmark report

USE FOR:
‚Ä¢ Getting reliable performance metrics
‚Ä¢ Comparing workflow optimizations
‚Ä¢ Eliminating one-off timing variations

TIPS:
‚Ä¢ Use 3+ runs for stable averages
‚Ä¢ First run is often slower (warmup)
‚Ä¢ Set delay to let GPU cool between runs"""

    CATEGORY = "‚ö° Performance Lab/Meta-Workflow"
    FUNCTION = "benchmark"
    RETURN_TYPES = ("FLOAT", "FLOAT", "FLOAT", "STRING")
    RETURN_NAMES = ("avg_duration", "min_duration", "max_duration", "report")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "runs": ("INT", {"default": 3, "min": 1, "max": 10}),
            },
            "optional": {
                "duration_1": ("FLOAT", {"default": 0.0}),
                "duration_2": ("FLOAT", {"default": 0.0}),
                "duration_3": ("FLOAT", {"default": 0.0}),
                "duration_4": ("FLOAT", {"default": 0.0}),
                "duration_5": ("FLOAT", {"default": 0.0}),
                "warmup_runs": ("INT", {"default": 1, "min": 0, "max": 3}),
                "label": ("STRING", {"default": "Benchmark"}),
            }
        }

    def benchmark(self, runs, duration_1=0.0, duration_2=0.0, duration_3=0.0,
                  duration_4=0.0, duration_5=0.0, warmup_runs=1, label="Benchmark"):
        # Collect all non-zero durations
        all_durations = [d for d in [duration_1, duration_2, duration_3, duration_4, duration_5] if d > 0]

        if not all_durations:
            return (0.0, 0.0, 0.0, "‚ùå No duration data - connect Performance Report outputs")

        # Skip warmup runs
        if warmup_runs > 0 and len(all_durations) > warmup_runs:
            durations = all_durations[warmup_runs:]
        else:
            durations = all_durations

        avg_dur = sum(durations) / len(durations)
        min_dur = min(durations)
        max_dur = max(durations)
        variance = max_dur - min_dur

        report = f"""‚ïê‚ïê‚ïê {label} Results ‚ïê‚ïê‚ïê

üìä Runs analyzed: {len(durations)} (after {warmup_runs} warmup)
‚è±Ô∏è  Average: {avg_dur:.2f}s
üöÄ Fastest: {min_dur:.2f}s
üê¢ Slowest: {max_dur:.2f}s
üìè Variance: {variance:.2f}s ({(variance/avg_dur*100):.1f}%)

Raw times: {', '.join(f'{d:.2f}s' for d in all_durations)}"""

        print(f"\n[Performance Lab]\n{report}\n")

        return (avg_dur, min_dur, max_dur, report)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# USER-FRIENDLY HELPER NODES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class PerfLab_OneClickOptimize:
    """One-Click Optimize - Everything in one simple node!"""

    DESCRIPTION = """‚ö° ONE-CLICK OPTIMIZE

THE EASIEST WAY TO OPTIMIZE!

Just connect your values and toggle Test Mode:
‚Ä¢ Test Mode ON = Fast testing (low res, few steps)
‚Ä¢ Test Mode OFF = Full quality production

INPUTS (connect from your workflow):
‚Ä¢ width/height: Your resolution
‚Ä¢ steps: Your step count
‚Ä¢ cfg: Your CFG value
‚Ä¢ batch_size: Your batch size

OUTPUTS (connect to your workflow):
‚Ä¢ All values optimized based on mode

ONE TOGGLE CONTROLS EVERYTHING!
No need to understand each optimization."""

    CATEGORY = "‚ö° Performance Lab/‚≠ê Start Here"
    FUNCTION = "optimize"
    RETURN_TYPES = ("INT", "INT", "INT", "FLOAT", "INT", "STRING")
    RETURN_NAMES = ("width", "height", "steps", "cfg", "batch_size", "status")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "test_mode": ("BOOLEAN", {"default": True,
                             "tooltip": "ON = fast testing, OFF = full quality"}),
            },
            "optional": {
                "width": ("INT", {"default": 1024, "min": 64, "max": 8192,
                         "tooltip": "Connect from Empty Latent or enter value"}),
                "height": ("INT", {"default": 1024, "min": 64, "max": 8192,
                          "tooltip": "Connect from Empty Latent or enter value"}),
                "steps": ("INT", {"default": 30, "min": 1, "max": 200,
                         "tooltip": "Connect from KSampler or enter value"}),
                "cfg": ("FLOAT", {"default": 7.0, "min": 0.0, "max": 30.0,
                       "tooltip": "Connect from KSampler or enter value"}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 64,
                              "tooltip": "Connect from Empty Latent or enter value"}),
                "test_resolution": ("INT", {"default": 512, "min": 256, "max": 1024,
                                   "tooltip": "Max resolution in test mode"}),
                "test_steps": ("INT", {"default": 15, "min": 4, "max": 50,
                              "tooltip": "Max steps in test mode"}),
            }
        }

    def optimize(self, test_mode, width=1024, height=1024, steps=30, cfg=7.0,
                 batch_size=1, test_resolution=512, test_steps=15):
        if test_mode:
            # Apply test optimizations
            if width > test_resolution or height > test_resolution:
                scale = test_resolution / max(width, height)
                width = int(width * scale) // 8 * 8
                height = int(height * scale) // 8 * 8
            steps = min(steps, test_steps)
            batch_size = 1
            status = f"üß™ TEST MODE: {width}x{height}, {steps} steps, batch=1"
        else:
            status = f"üé¨ PRODUCTION: {width}x{height}, {steps} steps, batch={batch_size}"

        print(f"[Performance Lab] {status}")
        return (width, height, steps, cfg, batch_size, status)


class PerfLab_QuickStart:
    """Quick Start Guide - Learn how to use Performance Lab."""

    DESCRIPTION = """üìö QUICK START GUIDE

ADD THIS NODE TO SEE INSTRUCTIONS!

This node outputs helpful documentation
about how to use Performance Lab.

Select a topic to learn about:
‚Ä¢ Getting Started - First steps
‚Ä¢ Speed Optimization - Make it faster
‚Ä¢ VRAM Optimization - Fix out of memory
‚Ä¢ Quality Optimization - Better images
‚Ä¢ Troubleshooting - Fix common issues"""

    CATEGORY = "‚ö° Performance Lab/‚≠ê Start Here"
    FUNCTION = "guide"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("guide",)
    OUTPUT_NODE = True

    GUIDES = {
        "Getting Started": """‚ïê‚ïê‚ïê GETTING STARTED ‚ïê‚ïê‚ïê

Welcome to Performance Lab! Here's how to begin:

1Ô∏è‚É£ ADD TIMING
   ‚Ä¢ Add ‚è±Ô∏è Start Timer at the START
   ‚Ä¢ Add üìä Performance Report at the END
   ‚Ä¢ Connect timer output to report

2Ô∏è‚É£ RUN YOUR WORKFLOW
   ‚Ä¢ Queue your workflow normally
   ‚Ä¢ Check console for timing results

3Ô∏è‚É£ OPTIMIZE
   ‚Ä¢ Add ‚ö° One-Click Optimize
   ‚Ä¢ Toggle Test Mode ON for fast testing
   ‚Ä¢ Toggle OFF when you want full quality

That's it! You're optimizing! üéâ""",

        "Speed Optimization": """‚ïê‚ïê‚ïê SPEED OPTIMIZATION ‚ïê‚ïê‚ïê

Make your workflow run FASTER:

üöÄ QUICK WINS:
‚Ä¢ Lower resolution (768 instead of 1024)
‚Ä¢ Fewer steps (15-20 for testing)
‚Ä¢ Batch size 1

üìê USE THESE NODES:
‚Ä¢ üìê Cap Resolution - Limit dimensions
‚Ä¢ üî¢ Reduce Steps - Limit step count
‚Ä¢ üöÄ Speed Test Preset - All at once

üí° TIPS:
‚Ä¢ Resolution affects speed quadratically
  (2x resolution = 4x slower)
‚Ä¢ Steps affect speed linearly
  (2x steps = 2x slower)
‚Ä¢ Test at 512px, render at 1024px""",

        "VRAM Optimization": """‚ïê‚ïê‚ïê VRAM OPTIMIZATION ‚ïê‚ïê‚ïê

Fix "CUDA out of memory" errors:

üíæ USE THESE NODES:
‚Ä¢ üíæ Low VRAM Preset - Select your GPU size
‚Ä¢ üì¶ Reduce Batch - Force batch=1
‚Ä¢ üìê Cap Resolution - Lower resolution

üéØ RECOMMENDED SETTINGS BY GPU:
‚Ä¢ 6GB:  512px max, 20 steps, batch=1
‚Ä¢ 8GB:  768px max, 25 steps, batch=1
‚Ä¢ 12GB: 1024px max, 30 steps, batch=1

üí° TIPS:
‚Ä¢ SDXL uses 2x VRAM of SD 1.5
‚Ä¢ Batch=4 uses 4x VRAM of batch=1
‚Ä¢ Upscalers add 2-4GB VRAM""",

        "Quality Optimization": """‚ïê‚ïê‚ïê QUALITY OPTIMIZATION ‚ïê‚ïê‚ïê

Get better image quality:

üé® KEY SETTINGS:
‚Ä¢ Use correct CFG for your model
‚Ä¢ Use enough steps (25-30 for quality)
‚Ä¢ Use native resolution

üéØ OPTIMAL CFG BY MODEL:
‚Ä¢ SD 1.5: 7.5
‚Ä¢ SDXL: 7.0
‚Ä¢ SD3: 4.5
‚Ä¢ Flux Dev: 3.5
‚Ä¢ Flux Schnell: 1.0

üìê OPTIMAL RESOLUTION:
‚Ä¢ SD 1.5: 512x512
‚Ä¢ SDXL/Flux: 1024x1024

üí° TIPS:
‚Ä¢ Higher steps = more detail (diminishing returns after 30)
‚Ä¢ Wrong CFG = black or burned images""",

        "Troubleshooting": """‚ïê‚ïê‚ïê TROUBLESHOOTING ‚ïê‚ïê‚ïê

Common issues and fixes:

üñ§ BLACK IMAGES?
‚Ä¢ Flux with CFG > 4? Lower to 1-3.5
‚Ä¢ Too few steps? Use at least 15-20
‚Ä¢ Use üîß Black Image Fix node

üí• OUT OF MEMORY?
‚Ä¢ Lower resolution
‚Ä¢ Reduce batch to 1
‚Ä¢ Use üíæ Low VRAM Preset

üêå TOO SLOW?
‚Ä¢ Lower resolution during testing
‚Ä¢ Use fewer steps (15-20)
‚Ä¢ Use üöÄ Speed Test Preset

‚ùì NODES NOT SHOWING?
‚Ä¢ Restart ComfyUI
‚Ä¢ Check console for errors
‚Ä¢ Reinstall from Manager""",
    }

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "topic": (list(cls.GUIDES.keys()),
                         {"tooltip": "Select a topic to learn about"}),
            }
        }

    def guide(self, topic):
        guide_text = self.GUIDES.get(topic, "Topic not found")
        print(f"\n[Performance Lab]\n{guide_text}\n")
        return (guide_text,)


class PerfLab_AutoDetectGPU:
    """Auto Detect GPU - Automatically detect your GPU and VRAM."""

    DESCRIPTION = """üîç AUTO DETECT GPU

Automatically detects your GPU and suggests settings!

NO INPUTS NEEDED - just add this node and run.

OUTPUTS:
‚Ä¢ gpu_name: Your GPU model
‚Ä¢ vram_gb: Total VRAM in GB
‚Ä¢ suggested_preset: Recommended VRAM preset
‚Ä¢ info: Full GPU information

USE THIS:
Connect 'suggested_preset' to Low VRAM Preset
to automatically use the right settings!"""

    CATEGORY = "‚ö° Performance Lab/‚≠ê Start Here"
    FUNCTION = "detect"
    RETURN_TYPES = ("STRING", "FLOAT", "STRING", "STRING")
    RETURN_NAMES = ("gpu_name", "vram_gb", "suggested_preset", "info")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {}}

    def detect(self):
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                total_vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                used_vram = torch.cuda.memory_allocated(0) / (1024**3)
                free_vram = total_vram - used_vram

                # Suggest preset based on VRAM
                if total_vram < 7:
                    preset = "6GB"
                elif total_vram < 10:
                    preset = "8GB"
                else:
                    preset = "12GB"

                info = f"""‚ïê‚ïê‚ïê GPU Detected ‚ïê‚ïê‚ïê
üñ•Ô∏è  GPU: {gpu_name}
üíæ Total VRAM: {total_vram:.1f} GB
üìä Used: {used_vram:.1f} GB
üìä Free: {free_vram:.1f} GB
üéØ Suggested Preset: {preset}"""

                print(f"\n[Performance Lab]\n{info}\n")
                return (gpu_name, total_vram, preset, info)
            else:
                return ("No CUDA GPU", 0.0, "8GB", "‚ùå No CUDA GPU detected")
        except Exception as e:
            return ("Error", 0.0, "8GB", f"‚ùå Error: {e}")


class PerfLab_ModelDetector:
    """Model Detector - Detect model type from checkpoint name."""

    DESCRIPTION = """üîç MODEL DETECTOR

Detects your model type from the checkpoint name!

HOW TO USE:
1. Enter or connect your checkpoint filename
2. Get detected model type and optimal settings

DETECTS:
‚Ä¢ SD 1.5 (sd15, v1-5, etc.)
‚Ä¢ SDXL (sdxl, xl, etc.)
‚Ä¢ SD3 (sd3, stable-diffusion-3)
‚Ä¢ Flux (flux, dev, schnell)

OUTPUTS:
‚Ä¢ model_type: Detected model
‚Ä¢ optimal_cfg: Best CFG for this model
‚Ä¢ optimal_resolution: Best resolution
‚Ä¢ info: Detection details"""

    CATEGORY = "‚ö° Performance Lab/‚≠ê Start Here"
    FUNCTION = "detect"
    RETURN_TYPES = ("STRING", "FLOAT", "INT", "STRING")
    RETURN_NAMES = ("model_type", "optimal_cfg", "optimal_resolution", "info")
    OUTPUT_NODE = True

    MODEL_PATTERNS = {
        "Flux Schnell": ["schnell", "flux-schnell"],
        "Flux Dev": ["flux", "flux-dev"],
        "SD3": ["sd3", "stable-diffusion-3"],
        "SDXL": ["sdxl", "xl-base", "xl_base"],
        "SD 1.5": ["sd15", "v1-5", "sd_1.5", "1.5"],
    }

    MODEL_SETTINGS = {
        "Flux Schnell": {"cfg": 1.0, "resolution": 1024},
        "Flux Dev": {"cfg": 3.5, "resolution": 1024},
        "SD3": {"cfg": 4.5, "resolution": 1024},
        "SDXL": {"cfg": 7.0, "resolution": 1024},
        "SD 1.5": {"cfg": 7.5, "resolution": 512},
        "Unknown": {"cfg": 7.0, "resolution": 768},
    }

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "checkpoint_name": ("STRING", {"default": "",
                                   "tooltip": "Enter checkpoint filename or connect from loader"}),
            }
        }

    def detect(self, checkpoint_name):
        name_lower = checkpoint_name.lower()
        detected = "Unknown"

        for model_type, patterns in self.MODEL_PATTERNS.items():
            for pattern in patterns:
                if pattern in name_lower:
                    detected = model_type
                    break
            if detected != "Unknown":
                break

        settings = self.MODEL_SETTINGS.get(detected, self.MODEL_SETTINGS["Unknown"])
        cfg = settings["cfg"]
        resolution = settings["resolution"]

        info = f"""‚ïê‚ïê‚ïê Model Detection ‚ïê‚ïê‚ïê
üìÅ Checkpoint: {checkpoint_name}
üéØ Detected: {detected}
‚öôÔ∏è  Optimal CFG: {cfg}
üìê Optimal Resolution: {resolution}x{resolution}"""

        print(f"\n[Performance Lab]\n{info}\n")
        return (detected, cfg, resolution, info)


class PerfLab_TestModeToggle:
    """Test Mode Toggle - Simple on/off for your whole workflow."""

    DESCRIPTION = """üîò TEST MODE TOGGLE

THE SIMPLEST OPTIMIZATION!

Just ONE toggle that outputs TRUE or FALSE.
Connect this to enable/disable inputs on
other nodes throughout your workflow.

USE:
‚Ä¢ ON = Testing (connect to 'enabled' inputs)
‚Ä¢ OFF = Production

CONNECT TO:
‚Ä¢ Cap Resolution 'enabled'
‚Ä¢ Reduce Steps 'enabled'
‚Ä¢ Low VRAM Preset 'enabled'
‚Ä¢ Any other 'enabled' input

ONE TOGGLE CONTROLS YOUR WHOLE WORKFLOW!"""

    CATEGORY = "‚ö° Performance Lab/‚≠ê Start Here"
    FUNCTION = "toggle"
    RETURN_TYPES = ("BOOLEAN", "BOOLEAN", "STRING")
    RETURN_NAMES = ("test_mode", "production_mode", "status")
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "test_mode": ("BOOLEAN", {"default": True,
                             "tooltip": "ON = testing mode, OFF = production mode"}),
            }
        }

    def toggle(self, test_mode):
        if test_mode:
            status = "üß™ TEST MODE - Fast iteration, lower quality"
        else:
            status = "üé¨ PRODUCTION MODE - Full quality output"

        print(f"[Performance Lab] {status}")
        return (test_mode, not test_mode, status)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# NODE REGISTRATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

NODE_CLASS_MAPPINGS = {
    # ‚≠ê Start Here (most user-friendly)
    "PerfLab_OneClickOptimize": PerfLab_OneClickOptimize,
    "PerfLab_QuickStart": PerfLab_QuickStart,
    "PerfLab_TestModeToggle": PerfLab_TestModeToggle,
    "PerfLab_AutoDetectGPU": PerfLab_AutoDetectGPU,
    "PerfLab_ModelDetector": PerfLab_ModelDetector,

    # Monitoring
    "PerfLab_Timer": PerfLab_Timer,
    "PerfLab_Report": PerfLab_Report,
    "PerfLab_VRAMMonitor": PerfLab_VRAMMonitor,

    # Quick Optimize
    "PerfLab_CapResolution": PerfLab_CapResolution,
    "PerfLab_ReduceSteps": PerfLab_ReduceSteps,
    "PerfLab_ReduceBatch": PerfLab_ReduceBatch,
    "PerfLab_OptimizeCFG": PerfLab_OptimizeCFG,
    "PerfLab_SpeedPreset": PerfLab_SpeedPreset,
    "PerfLab_LowVRAMPreset": PerfLab_LowVRAMPreset,

    # Analysis
    "PerfLab_Analyzer": PerfLab_Analyzer,
    "PerfLab_BlackImageFix": PerfLab_BlackImageFix,
    "PerfLab_Compare": PerfLab_Compare,

    # LLM
    "PerfLab_GeneratePrompt": PerfLab_GeneratePrompt,
    "PerfLab_LLMClient": PerfLab_LLMClient,
    "PerfLab_AutoOptimize": PerfLab_AutoOptimize,
    "PerfLab_ApplyOrRevert": PerfLab_ApplyOrRevert,

    # Utility
    "PerfLab_ShowText": PerfLab_ShowText,
    "PerfLab_Switch": PerfLab_Switch,
    "PerfLab_IntSwitch": PerfLab_IntSwitch,
    "PerfLab_FloatSwitch": PerfLab_FloatSwitch,

    # Meta-Workflow (test other workflows)
    "PerfLab_LoadWorkflow": PerfLab_LoadWorkflow,
    "PerfLab_QueueWorkflow": PerfLab_QueueWorkflow,
    "PerfLab_BenchmarkRunner": PerfLab_BenchmarkRunner,

    # Network (distributed services)
    "PerfLab_EndpointHealth": PerfLab_EndpointHealth,
    "PerfLab_NetworkScanner": PerfLab_NetworkScanner,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    # ‚≠ê Start Here (most user-friendly - shown first!)
    "PerfLab_OneClickOptimize": "‚ö° One-Click Optimize",
    "PerfLab_QuickStart": "üìö Quick Start Guide",
    "PerfLab_TestModeToggle": "üîò Test Mode Toggle",
    "PerfLab_AutoDetectGPU": "üîç Auto Detect GPU",
    "PerfLab_ModelDetector": "üîç Model Detector",

    # Monitoring
    "PerfLab_Timer": "‚è±Ô∏è Start Timer",
    "PerfLab_Report": "üìä Performance Report",
    "PerfLab_VRAMMonitor": "üíæ VRAM Monitor",

    # Quick Optimize
    "PerfLab_CapResolution": "üìê Cap Resolution",
    "PerfLab_ReduceSteps": "üî¢ Reduce Steps",
    "PerfLab_ReduceBatch": "üì¶ Reduce Batch",
    "PerfLab_OptimizeCFG": "üéØ Optimize CFG",
    "PerfLab_SpeedPreset": "üöÄ Speed Test Preset",
    "PerfLab_LowVRAMPreset": "üíæ Low VRAM Preset",

    # Analysis
    "PerfLab_Analyzer": "üîç Workflow Analyzer",
    "PerfLab_BlackImageFix": "üîß Black Image Fix",
    "PerfLab_Compare": "üìä Compare Results",

    # LLM
    "PerfLab_GeneratePrompt": "ü§ñ Generate LLM Prompt",
    "PerfLab_LLMClient": "ü§ñ LLM Client",
    "PerfLab_AutoOptimize": "ü§ñ Auto Optimize (LLM)",
    "PerfLab_ApplyOrRevert": "‚úÖ Apply or Revert",

    # Utility
    "PerfLab_ShowText": "üìù Show Text",
    "PerfLab_Switch": "üîÄ A/B Switch",
    "PerfLab_IntSwitch": "üî¢ Int A/B Switch",
    "PerfLab_FloatSwitch": "üî¢ Float A/B Switch",

    # Meta-Workflow
    "PerfLab_LoadWorkflow": "üìÇ Load Workflow",
    "PerfLab_QueueWorkflow": "‚ñ∂Ô∏è Queue Workflow",
    "PerfLab_BenchmarkRunner": "üèÅ Benchmark Runner",

    # Network
    "PerfLab_EndpointHealth": "üè• Endpoint Health",
    "PerfLab_NetworkScanner": "üîç Network Scanner",
}

# Print startup message
print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          ‚ö° Performance Lab v{__version__} Loaded! ‚ö°               ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  {len(NODE_CLASS_MAPPINGS)} nodes in "‚ö° Performance Lab" category:               ‚ïë
‚ïë                                                              ‚ïë
‚ïë  ‚≠ê START HERE:    One-Click Optimize, Quick Start Guide     ‚ïë
‚ïë  üìä Monitoring:    Timer, Report, VRAM Monitor               ‚ïë
‚ïë  üöÄ Optimize:      Cap Res, Steps, Batch, CFG, Presets       ‚ïë
‚ïë  üîç Analysis:      Analyzer, Black Image Fix, Compare        ‚ïë
‚ïë  ü§ñ LLM:           Generate Prompt, LLM Client, Auto Optimize ‚ïë
‚ïë  üîß Utility:       Show Text, A/B Switches                   ‚ïë
‚ïë  üìÇ Meta-Workflow: Load, Queue, Benchmark                    ‚ïë
‚ïë  üåê Network:       Health Check, Scanner                     ‚ïë
‚ïë                                                              ‚ïë
‚ïë  üí° New? Add "üìö Quick Start Guide" node for instructions!   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")
